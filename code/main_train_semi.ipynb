{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYhaiaGT6O5G","executionInfo":{"status":"ok","timestamp":1764985967083,"user_tz":300,"elapsed":21694,"user":{"displayName":"Xinyue Bei","userId":"07296435162047407787"}},"outputId":"62b411cc-a9c3-4e68-b91b-0c42280bb89c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlP0O7UP6NdQ","outputId":"f2e9703f-b330-477d-d274-f0604673248a","executionInfo":{"status":"ok","timestamp":1764987564703,"user_tz":300,"elapsed":148098,"user":{"displayName":"Xinyue Bei","userId":"07296435162047407787"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","[TRAIN] beta=0.5, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0124 | MSE: 0.0101 | PEHE: 0.0220 (Best: 0.0217)\n","[baseline run=0] Ep 100 | Loss: 0.0115 | MSE: 0.0102 | PEHE: 0.0307 (Best: 0.0215)\n","[baseline run=0] Ep 150 | Loss: 0.0111 | MSE: 0.0101 | PEHE: 0.0215 (Best: 0.0214)\n","[baseline run=0] Ep 168 | Loss: 0.0110 | MSE: 0.0100 | PEHE: 0.0216 (Best: 0.0214)\n","Early stopping at epoch 168\n","[SAVE] Run 0 complete. Test PEHE: 0.0228\n","\n","[TRAIN] beta=0.5, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.1298 | MSE: 0.0101 | PEHE: 0.0270 (Best: 0.0215)\n","[baseline_bal run=0] Ep 75 | Loss: 0.1605 | MSE: 0.0101 | PEHE: 0.0311 (Best: 0.0215)\n","Early stopping at epoch 75\n","[SAVE] Run 0 complete. Test PEHE: 0.0324\n","\n","[TRAIN] beta=0.5, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=0\n","[Propensity beta=0.5, l=0] Epoch 10/50, train_loss=0.4606, val_loss=0.4443 (Best: 0.4439)\n","[Propensity beta=0.5, l=0] Epoch 20/50, train_loss=0.4586, val_loss=0.4446 (Best: 0.4439)\n","[Propensity beta=0.5, l=0] Epoch 30/50, train_loss=0.4589, val_loss=0.4444 (Best: 0.4438)\n","[Propensity beta=0.5, l=0] Epoch 40/50, train_loss=0.4564, val_loss=0.4450 (Best: 0.4438)\n","[Propensity beta=0.5, l=0] Epoch 50/50, train_loss=0.4558, val_loss=0.4448 (Best: 0.4438)\n","[Propensity] Restored best model with val_loss=0.4438\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta0.5_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0129 | MSE: 0.0100 | PEHE: 0.0219 (Best: 0.0219)\n","[propensity run=0] Ep 100 | Loss: 0.0122 | MSE: 0.0101 | PEHE: 0.0233 (Best: 0.0216)\n","[propensity run=0] Ep 150 | Loss: 0.0114 | MSE: 0.0100 | PEHE: 0.0231 (Best: 0.0215)\n","[propensity run=0] Ep 200 | Loss: 0.0108 | MSE: 0.0101 | PEHE: 0.0251 (Best: 0.0215)\n","[propensity run=0] Ep 250 | Loss: 0.0105 | MSE: 0.0101 | PEHE: 0.0224 (Best: 0.0215)\n","[propensity run=0] Ep 300 | Loss: 0.0106 | MSE: 0.0100 | PEHE: 0.0215 (Best: 0.0214)\n","[propensity run=0] Ep 328 | Loss: 0.0104 | MSE: 0.0101 | PEHE: 0.0218 (Best: 0.0214)\n","Early stopping at epoch 328\n","[SAVE] Run 0 complete. Test PEHE: 0.0228\n","\n","[TRAIN] beta=0.5, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.9832 | MSE: 0.9612 | PEHE: 0.4770 (Best: 0.2466)\n","[baseline run=0] Ep 53 | Loss: 0.9851 | MSE: 0.9624 | PEHE: 0.5555 (Best: 0.2466)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.5569\n","\n","[TRAIN] beta=0.5, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 1.1043 | MSE: 0.9614 | PEHE: 0.4987 (Best: 0.0431)\n","[baseline_bal run=0] Ep 53 | Loss: 1.1178 | MSE: 0.9629 | PEHE: 0.5786 (Best: 0.0431)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.5798\n","\n","[TRAIN] beta=0.5, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=1\n","[Propensity beta=0.5, l=1] Epoch 10/50, train_loss=0.4760, val_loss=0.4463 (Best: 0.4421)\n","[Propensity beta=0.5, l=1] Epoch 20/50, train_loss=0.4728, val_loss=0.4492 (Best: 0.4421)\n","[Propensity beta=0.5, l=1] Epoch 30/50, train_loss=0.4718, val_loss=0.4437 (Best: 0.4421)\n","[Propensity beta=0.5, l=1] Epoch 40/50, train_loss=0.4726, val_loss=0.4432 (Best: 0.4421)\n","[Propensity beta=0.5, l=1] Epoch 50/50, train_loss=0.4695, val_loss=0.4441 (Best: 0.4421)\n","[Propensity] Restored best model with val_loss=0.4421\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta0.5_l1.pt\n","[propensity run=0] Ep 50 | Loss: 0.9787 | MSE: 0.9611 | PEHE: 0.4896 (Best: 0.3156)\n","[propensity run=0] Ep 53 | Loss: 0.9826 | MSE: 0.9613 | PEHE: 0.5144 (Best: 0.3156)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.5156\n","\n","[TRAIN] beta=0.5, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 1.9223 | MSE: 1.7776 | PEHE: 0.9532 (Best: 0.4601)\n","[baseline run=0] Ep 53 | Loss: 1.9148 | MSE: 1.7801 | PEHE: 0.7903 (Best: 0.4601)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.7905\n","\n","[TRAIN] beta=0.5, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 2.0558 | MSE: 1.7748 | PEHE: 0.8795 (Best: 0.0231)\n","[baseline_bal run=0] Ep 53 | Loss: 2.0714 | MSE: 1.7756 | PEHE: 0.8503 (Best: 0.0231)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.8514\n","\n","[TRAIN] beta=0.5, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=2\n","[Propensity beta=0.5, l=2] Epoch 10/50, train_loss=0.4753, val_loss=0.5034 (Best: 0.5034)\n","[Propensity beta=0.5, l=2] Epoch 20/50, train_loss=0.4746, val_loss=0.5056 (Best: 0.5030)\n","[Propensity beta=0.5, l=2] Epoch 30/50, train_loss=0.4730, val_loss=0.5051 (Best: 0.5030)\n","[Propensity beta=0.5, l=2] Epoch 40/50, train_loss=0.4728, val_loss=0.5042 (Best: 0.5023)\n","[Propensity beta=0.5, l=2] Epoch 50/50, train_loss=0.4715, val_loss=0.5085 (Best: 0.5022)\n","[Propensity] Restored best model with val_loss=0.5022\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta0.5_l2.pt\n","[propensity run=0] Ep 50 | Loss: 1.9112 | MSE: 1.7805 | PEHE: 0.7773 (Best: 0.3562)\n","[propensity run=0] Ep 51 | Loss: 1.9168 | MSE: 1.7764 | PEHE: 0.9280 (Best: 0.3562)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 0.9285\n","\n","[TRAIN] beta=0.5, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 3.4819 | MSE: 3.4184 | PEHE: 1.7959 (Best: 0.3554)\n","[baseline run=0] Ep 51 | Loss: 3.4624 | MSE: 3.4205 | PEHE: 1.7339 (Best: 0.3554)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 1.7353\n","\n","[TRAIN] beta=0.5, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 3.5787 | MSE: 3.4242 | PEHE: 1.6717 (Best: 0.0432)\n","[baseline_bal run=0] Ep 53 | Loss: 3.6541 | MSE: 3.4183 | PEHE: 1.7790 (Best: 0.0432)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 1.7798\n","\n","[TRAIN] beta=0.5, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=4\n","[Propensity beta=0.5, l=4] Epoch 10/50, train_loss=0.5120, val_loss=0.5083 (Best: 0.5083)\n","[Propensity beta=0.5, l=4] Epoch 20/50, train_loss=0.5100, val_loss=0.5076 (Best: 0.5075)\n","[Propensity beta=0.5, l=4] Epoch 30/50, train_loss=0.5109, val_loss=0.5074 (Best: 0.5070)\n","[Propensity beta=0.5, l=4] Epoch 40/50, train_loss=0.5087, val_loss=0.5063 (Best: 0.5063)\n","[Propensity beta=0.5, l=4] Epoch 50/50, train_loss=0.5089, val_loss=0.5072 (Best: 0.5058)\n","[Propensity] Restored best model with val_loss=0.5058\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta0.5_l4.pt\n","[propensity run=0] Ep 50 | Loss: 3.4817 | MSE: 3.4228 | PEHE: 1.6892 (Best: 0.2033)\n","[propensity run=0] Ep 51 | Loss: 3.4651 | MSE: 3.4267 | PEHE: 1.6409 (Best: 0.2033)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 1.6414\n","\n","[TRAIN] beta=0.5, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 6.3051 | MSE: 6.7500 | PEHE: 3.2527 (Best: 0.3003)\n","[baseline run=0] Ep 51 | Loss: 6.2781 | MSE: 6.7394 | PEHE: 2.8790 (Best: 0.3003)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 2.8891\n","\n","[TRAIN] beta=0.5, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 6.3723 | MSE: 6.7330 | PEHE: 3.0135 (Best: 0.1939)\n","[baseline_bal run=0] Ep 52 | Loss: 6.4007 | MSE: 6.7251 | PEHE: 2.9062 (Best: 0.1939)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 2.9068\n","\n","[TRAIN] beta=0.5, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=8\n","[Propensity beta=0.5, l=8] Epoch 10/50, train_loss=0.5530, val_loss=0.5444 (Best: 0.5444)\n","[Propensity beta=0.5, l=8] Epoch 20/50, train_loss=0.5527, val_loss=0.5450 (Best: 0.5438)\n","[Propensity beta=0.5, l=8] Epoch 30/50, train_loss=0.5503, val_loss=0.5440 (Best: 0.5435)\n","[Propensity beta=0.5, l=8] Epoch 40/50, train_loss=0.5492, val_loss=0.5431 (Best: 0.5431)\n","[Propensity beta=0.5, l=8] Epoch 50/50, train_loss=0.5519, val_loss=0.5430 (Best: 0.5429)\n","[Propensity] Restored best model with val_loss=0.5429\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta0.5_l8.pt\n","[propensity run=0] Ep 50 | Loss: 6.3018 | MSE: 6.7473 | PEHE: 2.7562 (Best: 0.2742)\n","[propensity run=0] Ep 51 | Loss: 6.2745 | MSE: 6.7637 | PEHE: 3.4474 (Best: 0.2742)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 3.4554\n","\n","[TRAIN] beta=1.0, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0119 | MSE: 0.0102 | PEHE: 0.0257 (Best: 0.0236)\n","[baseline run=0] Ep 100 | Loss: 0.0117 | MSE: 0.0102 | PEHE: 0.0292 (Best: 0.0227)\n","[baseline run=0] Ep 150 | Loss: 0.0114 | MSE: 0.0102 | PEHE: 0.0347 (Best: 0.0225)\n","[baseline run=0] Ep 165 | Loss: 0.0112 | MSE: 0.0102 | PEHE: 0.0231 (Best: 0.0225)\n","Early stopping at epoch 165\n","[SAVE] Run 0 complete. Test PEHE: 0.0230\n","\n","[TRAIN] beta=1.0, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.1506 | MSE: 0.0102 | PEHE: 0.0249 (Best: 0.0226)\n","[baseline_bal run=0] Ep 72 | Loss: 0.1495 | MSE: 0.0102 | PEHE: 0.0227 (Best: 0.0226)\n","Early stopping at epoch 72\n","[SAVE] Run 0 complete. Test PEHE: 0.0226\n","\n","[TRAIN] beta=1.0, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=0\n","[Propensity beta=1.0, l=0] Epoch 10/50, train_loss=0.4518, val_loss=0.4495 (Best: 0.4493)\n","[Propensity beta=1.0, l=0] Epoch 20/50, train_loss=0.4511, val_loss=0.4493 (Best: 0.4491)\n","[Propensity beta=1.0, l=0] Epoch 30/50, train_loss=0.4519, val_loss=0.4489 (Best: 0.4489)\n","[Propensity beta=1.0, l=0] Epoch 40/50, train_loss=0.4502, val_loss=0.4490 (Best: 0.4488)\n","[Propensity beta=1.0, l=0] Epoch 50/50, train_loss=0.4487, val_loss=0.4486 (Best: 0.4486)\n","[Propensity] Restored best model with val_loss=0.4486\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta1.0_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0117 | MSE: 0.0102 | PEHE: 0.0281 (Best: 0.0240)\n","[propensity run=0] Ep 100 | Loss: 0.0113 | MSE: 0.0102 | PEHE: 0.0258 (Best: 0.0227)\n","[propensity run=0] Ep 150 | Loss: 0.0111 | MSE: 0.0103 | PEHE: 0.0333 (Best: 0.0225)\n","[propensity run=0] Ep 200 | Loss: 0.0106 | MSE: 0.0102 | PEHE: 0.0242 (Best: 0.0225)\n","[propensity run=0] Ep 214 | Loss: 0.0107 | MSE: 0.0102 | PEHE: 0.0265 (Best: 0.0225)\n","Early stopping at epoch 214\n","[SAVE] Run 0 complete. Test PEHE: 0.0264\n","\n","[TRAIN] beta=1.0, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 3.6055 | MSE: 3.4970 | PEHE: 1.3657 (Best: 0.3278)\n","[baseline run=0] Ep 51 | Loss: 3.5851 | MSE: 3.4640 | PEHE: 1.8137 (Best: 0.3278)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 1.8151\n","\n","[TRAIN] beta=1.0, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 3.5626 | MSE: 3.4650 | PEHE: 1.6993 (Best: 0.2543)\n","[baseline_bal run=0] Ep 53 | Loss: 3.5707 | MSE: 3.4684 | PEHE: 1.7424 (Best: 0.2543)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 1.7422\n","\n","[TRAIN] beta=1.0, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=1\n","[Propensity beta=1.0, l=1] Epoch 10/50, train_loss=0.5244, val_loss=0.4905 (Best: 0.4887)\n","[Propensity beta=1.0, l=1] Epoch 20/50, train_loss=0.5232, val_loss=0.4906 (Best: 0.4886)\n","[Propensity beta=1.0, l=1] Epoch 30/50, train_loss=0.5236, val_loss=0.4882 (Best: 0.4882)\n","[Propensity beta=1.0, l=1] Epoch 40/50, train_loss=0.5231, val_loss=0.4886 (Best: 0.4882)\n","[Propensity beta=1.0, l=1] Epoch 50/50, train_loss=0.5207, val_loss=0.4903 (Best: 0.4882)\n","[Propensity] Restored best model with val_loss=0.4882\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta1.0_l1.pt\n","[propensity run=0] Ep 50 | Loss: 3.5732 | MSE: 3.4705 | PEHE: 1.6564 (Best: 0.4940)\n","[propensity run=0] Ep 51 | Loss: 3.5514 | MSE: 3.4655 | PEHE: 1.9292 (Best: 0.4940)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 1.9312\n","\n","[TRAIN] beta=1.0, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 6.3260 | MSE: 6.2855 | PEHE: 3.0310 (Best: 0.0347)\n","[baseline run=0] Ep 51 | Loss: 6.3339 | MSE: 6.2705 | PEHE: 2.9311 (Best: 0.0347)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 2.9267\n","\n","[TRAIN] beta=1.0, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 6.4238 | MSE: 6.2840 | PEHE: 3.0851 (Best: 0.1310)\n","[baseline_bal run=0] Ep 52 | Loss: 6.4006 | MSE: 6.2928 | PEHE: 3.1648 (Best: 0.1310)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 3.1651\n","\n","[TRAIN] beta=1.0, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=2\n","[Propensity beta=1.0, l=2] Epoch 10/50, train_loss=0.5462, val_loss=0.5248 (Best: 0.5247)\n","[Propensity beta=1.0, l=2] Epoch 20/50, train_loss=0.5457, val_loss=0.5243 (Best: 0.5243)\n","[Propensity beta=1.0, l=2] Epoch 30/50, train_loss=0.5427, val_loss=0.5255 (Best: 0.5239)\n","[Propensity beta=1.0, l=2] Epoch 40/50, train_loss=0.5419, val_loss=0.5241 (Best: 0.5238)\n","[Propensity beta=1.0, l=2] Epoch 50/50, train_loss=0.5414, val_loss=0.5233 (Best: 0.5233)\n","[Propensity] Restored best model with val_loss=0.5233\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta1.0_l2.pt\n","[propensity run=0] Ep 50 | Loss: 6.3299 | MSE: 6.2764 | PEHE: 3.0427 (Best: 0.2104)\n","[propensity run=0] Ep 51 | Loss: 6.3569 | MSE: 6.2877 | PEHE: 3.1310 (Best: 0.2104)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 3.1269\n","\n","[TRAIN] beta=1.0, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 11.1203 | MSE: 10.7025 | PEHE: 4.9248 (Best: 0.0595)\n","[baseline run=0] Ep 51 | Loss: 11.1834 | MSE: 10.7577 | PEHE: 4.7169 (Best: 0.0595)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 4.7141\n","\n","[TRAIN] beta=1.0, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 11.2164 | MSE: 10.7010 | PEHE: 4.9780 (Best: 0.1955)\n","[baseline_bal run=0] Ep 52 | Loss: 11.2277 | MSE: 10.6941 | PEHE: 5.1614 (Best: 0.1955)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 5.1605\n","\n","[TRAIN] beta=1.0, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=4\n","[Propensity beta=1.0, l=4] Epoch 10/50, train_loss=0.5815, val_loss=0.5886 (Best: 0.5884)\n","[Propensity beta=1.0, l=4] Epoch 20/50, train_loss=0.5809, val_loss=0.5889 (Best: 0.5884)\n","[Propensity beta=1.0, l=4] Epoch 30/50, train_loss=0.5789, val_loss=0.5889 (Best: 0.5884)\n","[Propensity beta=1.0, l=4] Epoch 40/50, train_loss=0.5828, val_loss=0.5888 (Best: 0.5884)\n","[Propensity beta=1.0, l=4] Epoch 50/50, train_loss=0.5824, val_loss=0.5926 (Best: 0.5877)\n","[Propensity] Restored best model with val_loss=0.5877\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta1.0_l4.pt\n","[propensity run=0] Ep 50 | Loss: 11.0828 | MSE: 10.8254 | PEHE: 4.4787 (Best: 0.1776)\n","[propensity run=0] Ep 51 | Loss: 11.1659 | MSE: 10.6737 | PEHE: 5.3012 (Best: 0.1776)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 5.2976\n","\n","[TRAIN] beta=1.0, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 18.4706 | MSE: 19.2525 | PEHE: 8.0050 (Best: 0.0804)\n","[baseline run=0] Ep 51 | Loss: 18.3660 | MSE: 19.2752 | PEHE: 7.4540 (Best: 0.0804)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 7.4494\n","\n","[TRAIN] beta=1.0, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 18.5203 | MSE: 19.2600 | PEHE: 7.6668 (Best: 0.0919)\n","[baseline_bal run=0] Ep 52 | Loss: 18.6051 | MSE: 19.2907 | PEHE: 7.5139 (Best: 0.0919)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 7.5150\n","\n","[TRAIN] beta=1.0, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=8\n","[Propensity beta=1.0, l=8] Epoch 10/50, train_loss=0.6195, val_loss=0.6204 (Best: 0.6202)\n","[Propensity beta=1.0, l=8] Epoch 20/50, train_loss=0.6204, val_loss=0.6213 (Best: 0.6198)\n","[Propensity beta=1.0, l=8] Epoch 30/50, train_loss=0.6188, val_loss=0.6200 (Best: 0.6198)\n","[Propensity beta=1.0, l=8] Epoch 40/50, train_loss=0.6169, val_loss=0.6203 (Best: 0.6198)\n","[Propensity beta=1.0, l=8] Epoch 50/50, train_loss=0.6164, val_loss=0.6204 (Best: 0.6198)\n","[Propensity] Restored best model with val_loss=0.6198\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta1.0_l8.pt\n","[propensity run=0] Ep 50 | Loss: 18.5136 | MSE: 19.2457 | PEHE: 7.6755 (Best: 0.5372)\n","[propensity run=0] Ep 51 | Loss: 18.3900 | MSE: 19.2449 | PEHE: 7.9019 (Best: 0.5372)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 7.9018\n","\n","[TRAIN] beta=2.0, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0125 | MSE: 0.0106 | PEHE: 0.0251 (Best: 0.0233)\n","[baseline run=0] Ep 100 | Loss: 0.0115 | MSE: 0.0106 | PEHE: 0.0308 (Best: 0.0227)\n","[baseline run=0] Ep 116 | Loss: 0.0116 | MSE: 0.0105 | PEHE: 0.0227 (Best: 0.0227)\n","Early stopping at epoch 116\n","[SAVE] Run 0 complete. Test PEHE: 0.0223\n","\n","[TRAIN] beta=2.0, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.1217 | MSE: 0.0105 | PEHE: 0.0249 (Best: 0.0224)\n","[baseline_bal run=0] Ep 100 | Loss: 0.0114 | MSE: 0.0105 | PEHE: 0.0247 (Best: 0.0224)\n","[baseline_bal run=0] Ep 147 | Loss: 0.0111 | MSE: 0.0106 | PEHE: 0.0316 (Best: 0.0224)\n","Early stopping at epoch 147\n","[SAVE] Run 0 complete. Test PEHE: 0.0314\n","\n","[TRAIN] beta=2.0, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=0\n","[Propensity beta=2.0, l=0] Epoch 10/50, train_loss=0.4714, val_loss=0.4109 (Best: 0.4073)\n","[Propensity beta=2.0, l=0] Epoch 20/50, train_loss=0.4705, val_loss=0.4106 (Best: 0.4073)\n","[Propensity beta=2.0, l=0] Epoch 30/50, train_loss=0.4694, val_loss=0.4086 (Best: 0.4073)\n","[Propensity beta=2.0, l=0] Epoch 40/50, train_loss=0.4698, val_loss=0.4118 (Best: 0.4073)\n","[Propensity beta=2.0, l=0] Epoch 50/50, train_loss=0.4691, val_loss=0.4114 (Best: 0.4070)\n","[Propensity] Restored best model with val_loss=0.4070\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta2.0_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0123 | MSE: 0.0107 | PEHE: 0.0425 (Best: 0.0236)\n","[propensity run=0] Ep 100 | Loss: 0.0114 | MSE: 0.0106 | PEHE: 0.0290 (Best: 0.0226)\n","[propensity run=0] Ep 150 | Loss: 0.0112 | MSE: 0.0105 | PEHE: 0.0234 (Best: 0.0225)\n","[propensity run=0] Ep 200 | Loss: 0.0106 | MSE: 0.0105 | PEHE: 0.0301 (Best: 0.0224)\n","[propensity run=0] Ep 250 | Loss: 0.0104 | MSE: 0.0105 | PEHE: 0.0228 (Best: 0.0223)\n","[propensity run=0] Ep 300 | Loss: 0.0103 | MSE: 0.0105 | PEHE: 0.0225 (Best: 0.0223)\n","[propensity run=0] Ep 349 | Loss: 0.0102 | MSE: 0.0105 | PEHE: 0.0223 (Best: 0.0223)\n","Early stopping at epoch 349\n","[SAVE] Run 0 complete. Test PEHE: 0.0221\n","\n","[TRAIN] beta=2.0, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 10.7110 | MSE: 11.7038 | PEHE: 5.5735 (Best: 0.0364)\n","[baseline run=0] Ep 51 | Loss: 10.7115 | MSE: 11.6842 | PEHE: 5.0446 (Best: 0.0364)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 5.0326\n","\n","[TRAIN] beta=2.0, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 10.7740 | MSE: 11.7110 | PEHE: 4.9973 (Best: 0.0290)\n","[baseline_bal run=0] Ep 52 | Loss: 10.7616 | MSE: 11.7286 | PEHE: 4.9693 (Best: 0.0290)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 4.9690\n","\n","[TRAIN] beta=2.0, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=1\n","[Propensity beta=2.0, l=1] Epoch 10/50, train_loss=0.5822, val_loss=0.5768 (Best: 0.5767)\n","[Propensity beta=2.0, l=1] Epoch 20/50, train_loss=0.5812, val_loss=0.5765 (Best: 0.5764)\n","[Propensity beta=2.0, l=1] Epoch 30/50, train_loss=0.5809, val_loss=0.5761 (Best: 0.5761)\n","[Propensity beta=2.0, l=1] Epoch 40/50, train_loss=0.5794, val_loss=0.5758 (Best: 0.5758)\n","[Propensity beta=2.0, l=1] Epoch 50/50, train_loss=0.5792, val_loss=0.5763 (Best: 0.5756)\n","[Propensity] Restored best model with val_loss=0.5756\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta2.0_l1.pt\n","[propensity run=0] Ep 50 | Loss: 10.6522 | MSE: 11.6926 | PEHE: 4.8085 (Best: 0.2074)\n","[propensity run=0] Ep 51 | Loss: 10.6436 | MSE: 11.6756 | PEHE: 5.2158 (Best: 0.2074)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 5.1982\n","\n","[TRAIN] beta=2.0, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 19.3209 | MSE: 17.0340 | PEHE: 8.1114 (Best: 0.2055)\n","[baseline run=0] Ep 51 | Loss: 19.3780 | MSE: 17.0212 | PEHE: 7.7668 (Best: 0.2055)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 7.7640\n","\n","[TRAIN] beta=2.0, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 19.3819 | MSE: 17.0426 | PEHE: 8.0102 (Best: 0.1153)\n","[baseline_bal run=0] Ep 52 | Loss: 19.4537 | MSE: 17.0668 | PEHE: 8.2470 (Best: 0.1153)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 8.2440\n","\n","[TRAIN] beta=2.0, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=2\n","[Propensity beta=2.0, l=2] Epoch 10/50, train_loss=0.6164, val_loss=0.6353 (Best: 0.6318)\n","[Propensity beta=2.0, l=2] Epoch 20/50, train_loss=0.6146, val_loss=0.6327 (Best: 0.6318)\n","[Propensity beta=2.0, l=2] Epoch 30/50, train_loss=0.6149, val_loss=0.6326 (Best: 0.6318)\n","[Propensity beta=2.0, l=2] Epoch 40/50, train_loss=0.6166, val_loss=0.6322 (Best: 0.6318)\n","[Propensity beta=2.0, l=2] Epoch 50/50, train_loss=0.6135, val_loss=0.6360 (Best: 0.6318)\n","[Propensity] Restored best model with val_loss=0.6318\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta2.0_l2.pt\n","[propensity run=0] Ep 50 | Loss: 19.3631 | MSE: 17.2780 | PEHE: 8.9521 (Best: 0.3893)\n","[propensity run=0] Ep 51 | Loss: 19.4140 | MSE: 17.1048 | PEHE: 7.1088 (Best: 0.3893)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 7.1099\n","\n","[TRAIN] beta=2.0, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 33.1427 | MSE: 31.8850 | PEHE: 11.2388 (Best: 0.2813)\n","[baseline run=0] Ep 51 | Loss: 33.1260 | MSE: 31.8500 | PEHE: 11.8102 (Best: 0.2813)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 11.7928\n","\n","[TRAIN] beta=2.0, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 33.0621 | MSE: 31.8116 | PEHE: 11.7878 (Best: 0.2761)\n","[baseline_bal run=0] Ep 52 | Loss: 33.2191 | MSE: 31.9491 | PEHE: 10.9166 (Best: 0.2761)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 10.9166\n","\n","[TRAIN] beta=2.0, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=4\n","[Propensity beta=2.0, l=4] Epoch 10/50, train_loss=0.6504, val_loss=0.6412 (Best: 0.6400)\n","[Propensity beta=2.0, l=4] Epoch 20/50, train_loss=0.6495, val_loss=0.6402 (Best: 0.6394)\n","[Propensity beta=2.0, l=4] Epoch 30/50, train_loss=0.6505, val_loss=0.6456 (Best: 0.6392)\n","[Propensity beta=2.0, l=4] Epoch 40/50, train_loss=0.6487, val_loss=0.6415 (Best: 0.6391)\n","[Propensity beta=2.0, l=4] Epoch 50/50, train_loss=0.6491, val_loss=0.6394 (Best: 0.6389)\n","[Propensity] Restored best model with val_loss=0.6389\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta2.0_l4.pt\n","[propensity run=0] Ep 50 | Loss: 32.9710 | MSE: 31.9069 | PEHE: 11.1428 (Best: 0.2407)\n","[propensity run=0] Ep 51 | Loss: 33.0555 | MSE: 31.8780 | PEHE: 11.0735 (Best: 0.2407)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 11.0584\n","\n","[TRAIN] beta=2.0, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 58.9875 | MSE: 56.7118 | PEHE: 17.7574 (Best: 0.0648)\n","[baseline run=0] Ep 51 | Loss: 58.6611 | MSE: 56.8258 | PEHE: 17.0497 (Best: 0.0648)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 17.0482\n","\n","[TRAIN] beta=2.0, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 58.8269 | MSE: 56.9454 | PEHE: 16.8869 (Best: 0.0368)\n","[baseline_bal run=0] Ep 52 | Loss: 58.7552 | MSE: 56.9764 | PEHE: 16.7449 (Best: 0.0368)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 16.7461\n","\n","[TRAIN] beta=2.0, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=8\n","[Propensity beta=2.0, l=8] Epoch 10/50, train_loss=0.6726, val_loss=0.6582 (Best: 0.6550)\n","[Propensity beta=2.0, l=8] Epoch 20/50, train_loss=0.6714, val_loss=0.6569 (Best: 0.6550)\n","[Propensity beta=2.0, l=8] Epoch 30/50, train_loss=0.6709, val_loss=0.6580 (Best: 0.6550)\n","[Propensity beta=2.0, l=8] Epoch 40/50, train_loss=0.6707, val_loss=0.6573 (Best: 0.6550)\n","[Propensity beta=2.0, l=8] Epoch 50/50, train_loss=0.6701, val_loss=0.6583 (Best: 0.6550)\n","[Propensity] Restored best model with val_loss=0.6550\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta2.0_l8.pt\n","[propensity run=0] Ep 50 | Loss: 59.1184 | MSE: 56.8587 | PEHE: 17.7108 (Best: 0.2958)\n","[propensity run=0] Ep 51 | Loss: 59.0419 | MSE: 56.7664 | PEHE: 17.1874 (Best: 0.2958)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 17.1840\n","\n","[TRAIN] beta=4.0, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0117 | MSE: 0.0115 | PEHE: 0.0250 (Best: 0.0244)\n","[baseline run=0] Ep 100 | Loss: 0.0114 | MSE: 0.0115 | PEHE: 0.0238 (Best: 0.0234)\n","[baseline run=0] Ep 131 | Loss: 0.0110 | MSE: 0.0115 | PEHE: 0.0344 (Best: 0.0234)\n","Early stopping at epoch 131\n","[SAVE] Run 0 complete. Test PEHE: 0.0342\n","\n","[TRAIN] beta=4.0, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.1510 | MSE: 0.0115 | PEHE: 0.0246 (Best: 0.0222)\n","[baseline_bal run=0] Ep 63 | Loss: 0.1419 | MSE: 0.0115 | PEHE: 0.0364 (Best: 0.0222)\n","Early stopping at epoch 63\n","[SAVE] Run 0 complete. Test PEHE: 0.0359\n","\n","[TRAIN] beta=4.0, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=0\n","[Propensity beta=4.0, l=0] Epoch 10/50, train_loss=0.4604, val_loss=0.4404 (Best: 0.4404)\n","[Propensity beta=4.0, l=0] Epoch 20/50, train_loss=0.4597, val_loss=0.4406 (Best: 0.4403)\n","[Propensity beta=4.0, l=0] Epoch 30/50, train_loss=0.4588, val_loss=0.4416 (Best: 0.4399)\n","[Propensity beta=4.0, l=0] Epoch 40/50, train_loss=0.4589, val_loss=0.4428 (Best: 0.4397)\n","[Propensity beta=4.0, l=0] Epoch 50/50, train_loss=0.4573, val_loss=0.4411 (Best: 0.4396)\n","[Propensity] Restored best model with val_loss=0.4396\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta4.0_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0121 | MSE: 0.0115 | PEHE: 0.0387 (Best: 0.0268)\n","[propensity run=0] Ep 100 | Loss: 0.0110 | MSE: 0.0115 | PEHE: 0.0457 (Best: 0.0229)\n","[propensity run=0] Ep 150 | Loss: 0.0108 | MSE: 0.0116 | PEHE: 0.0259 (Best: 0.0226)\n","[propensity run=0] Ep 200 | Loss: 0.0103 | MSE: 0.0115 | PEHE: 0.0228 (Best: 0.0223)\n","[propensity run=0] Ep 250 | Loss: 0.0102 | MSE: 0.0115 | PEHE: 0.0222 (Best: 0.0222)\n","[propensity run=0] Ep 285 | Loss: 0.0101 | MSE: 0.0115 | PEHE: 0.0227 (Best: 0.0222)\n","Early stopping at epoch 285\n","[SAVE] Run 0 complete. Test PEHE: 0.0230\n","\n","[TRAIN] beta=4.0, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 33.8080 | MSE: 32.7208 | PEHE: 10.9779 (Best: 0.2119)\n","[baseline run=0] Ep 51 | Loss: 33.5331 | MSE: 32.6170 | PEHE: 11.4412 (Best: 0.2119)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 11.4481\n","\n","[TRAIN] beta=4.0, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 33.6584 | MSE: 32.8025 | PEHE: 12.7192 (Best: 0.0421)\n","[baseline_bal run=0] Ep 52 | Loss: 34.2132 | MSE: 32.6124 | PEHE: 11.2147 (Best: 0.0421)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 11.2108\n","\n","[TRAIN] beta=4.0, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=1\n","[Propensity beta=4.0, l=1] Epoch 10/50, train_loss=0.6440, val_loss=0.6371 (Best: 0.6361)\n","[Propensity beta=4.0, l=1] Epoch 20/50, train_loss=0.6437, val_loss=0.6381 (Best: 0.6361)\n","[Propensity beta=4.0, l=1] Epoch 30/50, train_loss=0.6429, val_loss=0.6388 (Best: 0.6361)\n","[Propensity beta=4.0, l=1] Epoch 40/50, train_loss=0.6414, val_loss=0.6369 (Best: 0.6361)\n","[Propensity beta=4.0, l=1] Epoch 50/50, train_loss=0.6405, val_loss=0.6369 (Best: 0.6361)\n","[Propensity] Restored best model with val_loss=0.6361\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta4.0_l1.pt\n","[propensity run=0] Ep 50 | Loss: 33.8270 | MSE: 32.8113 | PEHE: 12.5785 (Best: 0.1800)\n","[propensity run=0] Ep 51 | Loss: 33.5023 | MSE: 32.6163 | PEHE: 11.7782 (Best: 0.1800)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 11.7743\n","\n","[TRAIN] beta=4.0, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 58.8893 | MSE: 59.0415 | PEHE: 17.9592 (Best: 0.1971)\n","[baseline run=0] Ep 51 | Loss: 58.7165 | MSE: 58.9203 | PEHE: 16.9802 (Best: 0.1971)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 17.0186\n","\n","[TRAIN] beta=4.0, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 58.1802 | MSE: 59.2619 | PEHE: 16.6319 (Best: 0.3709)\n","[baseline_bal run=0] Ep 51 | Loss: 57.8839 | MSE: 58.8975 | PEHE: 17.2467 (Best: 0.3709)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 17.2555\n","\n","[TRAIN] beta=4.0, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=2\n","[Propensity beta=4.0, l=2] Epoch 10/50, train_loss=0.6669, val_loss=0.6763 (Best: 0.6756)\n","[Propensity beta=4.0, l=2] Epoch 20/50, train_loss=0.6668, val_loss=0.6761 (Best: 0.6756)\n","[Propensity beta=4.0, l=2] Epoch 30/50, train_loss=0.6662, val_loss=0.6765 (Best: 0.6756)\n","[Propensity beta=4.0, l=2] Epoch 40/50, train_loss=0.6691, val_loss=0.6766 (Best: 0.6756)\n","[Propensity beta=4.0, l=2] Epoch 50/50, train_loss=0.6650, val_loss=0.6762 (Best: 0.6756)\n","[Propensity] Restored best model with val_loss=0.6756\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta4.0_l2.pt\n","[propensity run=0] Ep 50 | Loss: 58.2480 | MSE: 58.9415 | PEHE: 18.0379 (Best: 0.1322)\n","[propensity run=0] Ep 51 | Loss: 58.4730 | MSE: 59.1011 | PEHE: 16.4553 (Best: 0.1322)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 16.4708\n","\n","[TRAIN] beta=4.0, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 109.6954 | MSE: 101.5124 | PEHE: 26.3428 (Best: 0.3138)\n","[baseline run=0] Ep 51 | Loss: 109.5000 | MSE: 101.0192 | PEHE: 24.6015 (Best: 0.3138)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 24.6038\n","\n","[TRAIN] beta=4.0, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 110.5663 | MSE: 101.0416 | PEHE: 24.6225 (Best: 0.2822)\n","[baseline_bal run=0] Ep 51 | Loss: 109.6286 | MSE: 101.5788 | PEHE: 26.3548 (Best: 0.2822)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 26.3592\n","\n","[TRAIN] beta=4.0, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=4\n","[Propensity beta=4.0, l=4] Epoch 10/50, train_loss=0.6805, val_loss=0.6825 (Best: 0.6813)\n","[Propensity beta=4.0, l=4] Epoch 20/50, train_loss=0.6787, val_loss=0.6813 (Best: 0.6813)\n","[Propensity beta=4.0, l=4] Epoch 30/50, train_loss=0.6796, val_loss=0.6814 (Best: 0.6813)\n","[Propensity beta=4.0, l=4] Epoch 40/50, train_loss=0.6786, val_loss=0.6822 (Best: 0.6813)\n","[Propensity beta=4.0, l=4] Epoch 50/50, train_loss=0.6769, val_loss=0.6819 (Best: 0.6813)\n","[Propensity] Restored best model with val_loss=0.6813\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta4.0_l4.pt\n","[propensity run=0] Ep 50 | Loss: 110.7440 | MSE: 102.1833 | PEHE: 22.7169 (Best: 0.4214)\n","[propensity run=0] Ep 51 | Loss: 109.9709 | MSE: 101.2231 | PEHE: 23.7624 (Best: 0.4214)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 23.7655\n","\n","[TRAIN] beta=4.0, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 198.8379 | MSE: 202.6771 | PEHE: 35.1280 (Best: 0.0634)\n","[baseline run=0] Ep 51 | Loss: 198.6446 | MSE: 205.1038 | PEHE: 38.1049 (Best: 0.0634)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 38.0478\n","\n","[TRAIN] beta=4.0, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 199.4028 | MSE: 202.7798 | PEHE: 34.5006 (Best: 0.1199)\n","[baseline_bal run=0] Ep 51 | Loss: 202.1477 | MSE: 205.6403 | PEHE: 31.8241 (Best: 0.1199)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 31.7785\n","\n","[TRAIN] beta=4.0, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=8\n","[Propensity beta=4.0, l=8] Epoch 10/50, train_loss=0.6842, val_loss=0.6906 (Best: 0.6900)\n","[Propensity beta=4.0, l=8] Epoch 20/50, train_loss=0.6812, val_loss=0.6902 (Best: 0.6900)\n","[Propensity beta=4.0, l=8] Epoch 30/50, train_loss=0.6817, val_loss=0.6934 (Best: 0.6900)\n","[Propensity beta=4.0, l=8] Epoch 40/50, train_loss=0.6821, val_loss=0.6934 (Best: 0.6900)\n","[Propensity beta=4.0, l=8] Epoch 50/50, train_loss=0.6815, val_loss=0.6918 (Best: 0.6900)\n","[Propensity] Restored best model with val_loss=0.6900\n","[Propensity] Saved propensity model to /content/drive/MyDrive/Project/experiments_semi_synthetic/propensity_models/propensity_beta4.0_l8.pt\n","[propensity run=0] Ep 50 | Loss: 200.1006 | MSE: 203.2811 | PEHE: 34.0366 (Best: 0.2950)\n","[propensity run=0] Ep 51 | Loss: 200.6640 | MSE: 203.1101 | PEHE: 34.9071 (Best: 0.2950)\n","Early stopping at epoch 51\n","[SAVE] Run 0 complete. Test PEHE: 34.8323\n"]}],"source":["import os\n","import json\n","from typing import Tuple, Optional, Dict, List\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# ============================================================\n","# Paths and basic configuration\n","# ============================================================\n","\n","ROOT = \"/content/drive/MyDrive/Project\"\n","\n","USE_SEMI_SYNTH = True\n","\n","if USE_SEMI_SYNTH:\n","    DATA_DIR   = os.path.join(ROOT, \"experiments_semi_synthetic\", \"data\")\n","    MODEL_DIR  = os.path.join(ROOT, \"experiments_semi_synthetic\", \"models\")\n","    RESULT_DIR = os.path.join(ROOT, \"experiments_semi_synthetic\", \"results\")\n","    PROP_DIR   = os.path.join(ROOT, \"experiments_semi_synthetic\", \"propensity_models\")\n","    FNAME_FMT  = \"semi_beta{beta}_l{l}.pt\"\n","else:\n","    DATA_DIR   = os.path.join(ROOT, \"experiments_synthetic\", \"data\")\n","    MODEL_DIR  = os.path.join(ROOT, \"experiments_synthetic\", \"models\")\n","    RESULT_DIR = os.path.join(ROOT, \"experiments_synthetic\", \"results\")\n","    PROP_DIR   = os.path.join(ROOT, \"experiments_synthetic\", \"propensity_models\")\n","    FNAME_FMT  = \"synthetic_beta{beta}_l{l}.pt\"\n","\n","os.makedirs(DATA_DIR, exist_ok=True)\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","os.makedirs(RESULT_DIR, exist_ok=True)\n","os.makedirs(PROP_DIR, exist_ok=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Training hyperparameters\n","BATCH_SIZE = 256\n","LR_TARNET = 5e-4\n","LR_PROP = 1e-3\n","N_EPOCHS_TARNET = 1000  # Set high, we rely on Early Stopping now\n","N_EPOCHS_PROP = 50\n","PATIENCE = 50           # Early stopping patience\n","LAMBDA_BAL = 10.0       # Increased from 1.0 to force balancing with strong bias\n","\n","\n","# ============================================================\n","# Utilities: loading synthetic datasets\n","# ============================================================\n","\n","def dataset_path(beta_norm: float, l: int) -> str:\n","    return os.path.join(DATA_DIR, FNAME_FMT.format(beta=beta_norm, l=l))\n","\n","\n","def load_synthetic_dataset(beta_norm: float, l: int):\n","    path = dataset_path(beta_norm, l)\n","    if not os.path.isfile(path):\n","        raise FileNotFoundError(f\"Dataset not found: {path}\")\n","    data = torch.load(path, map_location=\"cpu\")\n","\n","    X = data[\"X\"].float()  # (N, 1, H, W)\n","    t = data[\"T\"].float().view(-1, 1)  # (N, 1)\n","    y = data[\"Y\"].float().view(-1, 1)  # (N, 1)\n","\n","    if \"tau\" in data:\n","        tau = data[\"tau\"].float().view(-1, 1)\n","    else:\n","        mu0 = data[\"mu0\"].float().view(-1, 1)\n","        mu1 = data[\"mu1\"].float().view(-1, 1)\n","        tau = mu1 - mu0\n","\n","    return X, t, y, tau\n","\n","\n","def train_val_test_split(\n","    X: torch.Tensor,\n","    t: torch.Tensor,\n","    y: torch.Tensor,\n","    tau: torch.Tensor,\n","    train_frac: float = 0.6,\n","    val_frac: float = 0.2,\n","    seed: int = 1234,\n","):\n","    \"\"\"\n","    Simple random split into train/val/test with reproducible seed.\n","    \"\"\"\n","    N = X.shape[0]\n","    assert N == t.shape[0] == y.shape[0] == tau.shape[0]\n","\n","    g = torch.Generator().manual_seed(seed)\n","    perm = torch.randperm(N, generator=g)\n","\n","    n_train = int(train_frac * N)\n","    n_val = int(val_frac * N)\n","    # n_test = N - n_train - n_val\n","\n","    idx_train = perm[:n_train]\n","    idx_val = perm[n_train:n_train + n_val]\n","    idx_test = perm[n_train + n_val:]\n","\n","    data_train = (X[idx_train], t[idx_train], y[idx_train], tau[idx_train])\n","    data_val   = (X[idx_val],   t[idx_val],   y[idx_val],   tau[idx_val])\n","    data_test  = (X[idx_test],  t[idx_test],  y[idx_test],  tau[idx_test])\n","\n","    return data_train, data_val, data_test\n","\n","\n","# ============================================================\n","# Dataset object\n","# ============================================================\n","\n","class CausalDataset(Dataset):\n","    def __init__(\n","        self,\n","        X: torch.Tensor,\n","        t: torch.Tensor,\n","        y: torch.Tensor,\n","        tau: torch.Tensor,\n","        e_hat: Optional[torch.Tensor] = None,\n","    ):\n","        self.X = X\n","        self.t = t\n","        self.y = y\n","        self.tau = tau\n","        self.e_hat = e_hat\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = self.X[idx]\n","        t = self.t[idx]\n","        y = self.y[idx]\n","        tau = self.tau[idx]\n","        if self.e_hat is None:\n","            e = torch.tensor([0.0], dtype=torch.float32)  # dummy\n","        else:\n","            e = self.e_hat[idx]\n","        return x, t, y, tau, e\n","\n","\n","# ============================================================\n","# Models: representation, TARNet, propensity model\n","# ============================================================\n","\n","class ConvRepresentation(nn.Module):\n","    def __init__(self, rep_dim: int = 64):\n","        super().__init__()\n","        # === CHANGE 1: SIMPLER CNN ===\n","        # Reduced filters from [16, 32, 64] to [8, 16, 32] to reduce capacity\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(32, rep_dim), # Input matches last conv channel count\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x):\n","        h = self.features(x)\n","        h = self.fc(h)\n","        return h  # (N, rep_dim)\n","\n","\n","class TARNet(nn.Module):\n","    def __init__(self, rep_dim: int = 64, use_propensity: bool = False):\n","        super().__init__()\n","        self.use_propensity = use_propensity\n","        self.rep = ConvRepresentation(rep_dim=rep_dim)\n","\n","        in_dim_heads = rep_dim + 1 if use_propensity else rep_dim\n","\n","        # === CHANGE 2: ADD DROPOUT ===\n","        # Added Dropout(0.3) to prevent overfitting in the heads\n","        self.head0 = nn.Sequential(\n","            nn.Linear(in_dim_heads, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1),\n","        )\n","        self.head1 = nn.Sequential(\n","            nn.Linear(in_dim_heads, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1),\n","        )\n","\n","    def forward(self, x, e_hat=None):\n","        phi_x = self.rep(x)\n","        if self.use_propensity:\n","            if e_hat is None:\n","                raise ValueError(\"e_hat must be provided when use_propensity=True\")\n","            inp = torch.cat([phi_x, e_hat], dim=1)\n","        else:\n","            inp = phi_x\n","        y0 = self.head0(inp)\n","        y1 = self.head1(inp)\n","        return y0, y1, phi_x\n","\n","\n","class PropensityMLP(nn.Module):\n","    \"\"\"\n","    Simple propensity model using flattened images -> 2-layer MLP.\n","    \"\"\"\n","    def __init__(self, input_dim: int):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1),\n","        )\n","\n","    def forward(self, x):\n","        flat = x.view(x.size(0), -1)\n","        logits = self.net(flat)\n","        return logits\n","\n","\n","# ============================================================\n","# Losses and metrics\n","# ============================================================\n","\n","def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n","    \"\"\"Compute pairwise euclidean distance matrix\"\"\"\n","    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n","    norm = float(norm)\n","    expansion_1 = sample_1.unsqueeze(1).expand(n_1, n_2, -1)\n","    expansion_2 = sample_2.unsqueeze(0).expand(n_1, n_2, -1)\n","    return torch.norm(expansion_1 - expansion_2, p=norm, dim=2)\n","\n","def balancing_loss(phi_x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    RBF MMD Loss with Median Heuristic.\n","    \"\"\"\n","    t = t.view(-1)\n","    treated = phi_x[t > 0.5]\n","    control = phi_x[t <= 0.5]\n","\n","    if treated.size(0) == 0 or control.size(0) == 0:\n","        return torch.tensor(0.0, device=phi_x.device)\n","\n","    # 1. Dynamic Sigma Calculation\n","    all_samples = torch.cat([treated, control], dim=0)\n","    dists = pdist(all_samples, all_samples)\n","\n","    n = dists.shape[0]\n","    mask = ~torch.eye(n, dtype=torch.bool, device=phi_x.device)\n","    median_dist = dists[mask].median()\n","\n","    sigma = median_dist.detach()\n","    if sigma == 0: sigma = torch.tensor(1.0, device=phi_x.device)\n","\n","    # 2. Compute MMD\n","    D_tt = pdist(treated, treated)\n","    D_tc = pdist(treated, control)\n","    D_cc = pdist(control, control)\n","\n","    K_tt = torch.exp(-D_tt**2 / (2 * sigma**2)).mean()\n","    K_tc = torch.exp(-D_tc**2 / (2 * sigma**2)).mean()\n","    K_cc = torch.exp(-D_cc**2 / (2 * sigma**2)).mean()\n","\n","    return K_tt - 2 * K_tc + K_cc\n","\n","\n","\n","def compute_pehe(model: TARNet, dataloader: DataLoader, use_propensity: bool) -> float:\n","    \"\"\"\n","    Compute PEHE on a given dataloader.\n","    \"\"\"\n","    model.eval()\n","    sq_errors = []\n","\n","    with torch.no_grad():\n","        for x, t, y, tau, e_hat in dataloader:\n","            x = x.to(device)\n","            tau = tau.to(device)\n","            e_hat = e_hat.to(device)\n","\n","            if use_propensity:\n","                y0_hat, y1_hat, _ = model(x, e_hat=e_hat)\n","            else:\n","                y0_hat, y1_hat, _ = model(x, e_hat=None)\n","\n","            tau_hat = (y1_hat - y0_hat)\n","            sq_errors.append((tau_hat - tau) ** 2)\n","\n","    if len(sq_errors) == 0:\n","        return float(\"nan\")\n","\n","    sq_errors = torch.cat(sq_errors, dim=0)\n","    pehe = torch.sqrt(sq_errors.mean()).item()\n","    return pehe\n","\n","\n","def compute_mse_factual(model: TARNet, dataloader: DataLoader, use_propensity: bool) -> float:\n","    model.eval()\n","    mse = nn.MSELoss(reduction=\"sum\")\n","    total_loss = 0.0\n","    total_n = 0\n","\n","    with torch.no_grad():\n","        for x, t, y, tau, e_hat in dataloader:\n","            x = x.to(device)\n","            t = t.to(device)\n","            y = y.to(device)\n","            e_hat = e_hat.to(device)\n","\n","            if use_propensity:\n","                y0_hat, y1_hat, _ = model(x, e_hat=e_hat)\n","            else:\n","                y0_hat, y1_hat, _ = model(x, e_hat=None)\n","\n","            y_hat = torch.where(t > 0.5, y1_hat, y0_hat)\n","            loss = mse(y_hat, y)\n","            total_loss += loss.item()\n","            total_n += y.shape[0]\n","\n","    if total_n == 0:\n","        return float(\"nan\")\n","    return total_loss / total_n\n","\n","\n","# ============================================================\n","# Training helpers\n","# ============================================================\n","\n","def get_or_train_propensity_model(\n","    beta_norm: float,\n","    l: int,\n","    train_dataset: CausalDataset,\n","    val_dataset: CausalDataset,\n","    img_size: Tuple[int, int],\n",") -> PropensityMLP:\n","    \"\"\"\n","    Train or load a propensity model.\n","    UPDATED: Uses Weight Decay, Lower LR, and Best Model Checkpointing.\n","    \"\"\"\n","    H, W = img_size\n","    input_dim = 1 * H * W\n","    prop_path = os.path.join(PROP_DIR, f\"propensity_beta{beta_norm}_l{l}.pt\")\n","\n","    prop_model = PropensityMLP(input_dim=input_dim).to(device)\n","\n","    # if os.path.isfile(prop_path):\n","    #     print(f\"[Propensity] Loading existing propensity model from {prop_path}\")\n","    #     state = torch.load(prop_path, map_location=device)\n","    #     prop_model.load_state_dict(state)\n","    #     return prop_model\n","\n","    print(f\"[Propensity] Training new propensity model for beta={beta_norm}, l={l}\")\n","\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # === CHANGE 1 & 2: Lower LR and Add Weight Decay ===\n","    # Reduced LR to 1e-4 for stability\n","    # Added weight_decay=1e-4 for regularization\n","    optimizer = torch.optim.Adam(prop_model.parameters(), lr=1e-4, weight_decay=1e-4)\n","    bce = nn.BCEWithLogitsLoss()\n","\n","    # === CHANGE 3: Track Best Model ===\n","    best_val_loss = float('inf')\n","    best_state = None\n","\n","    for epoch in range(N_EPOCHS_PROP):\n","        prop_model.train()\n","        total_loss = 0.0\n","        n = 0\n","\n","        for x, t, y, tau, e_hat in train_loader:\n","            x = x.to(device)\n","            t = t.to(device)\n","\n","            logits = prop_model(x)\n","            loss = bce(logits, t)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * x.size(0)\n","            n += x.size(0)\n","\n","        avg_train_loss = total_loss / n if n > 0 else 0.0\n","\n","        # Validation Loop\n","        prop_model.eval()\n","        total_val_loss = 0.0\n","        n_val = 0\n","        with torch.no_grad():\n","            for x, t, y, tau, e_hat in val_loader:\n","                x = x.to(device)\n","                t = t.to(device)\n","                logits = prop_model(x)\n","                loss = bce(logits, t)\n","                total_val_loss += loss.item() * x.size(0)\n","                n_val += x.size(0)\n","\n","        avg_val_loss = total_val_loss / n_val if n_val > 0 else 0.0\n","\n","        # Save Best State\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            best_state = prop_model.state_dict()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(\n","                f\"[Propensity beta={beta_norm}, l={l}] \"\n","                f\"Epoch {epoch+1}/{N_EPOCHS_PROP}, \"\n","                f\"train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f} \"\n","                f\"(Best: {best_val_loss:.4f})\"\n","            )\n","\n","    # === CRITICAL: Restore Best State ===\n","    if best_state is not None:\n","        prop_model.load_state_dict(best_state)\n","        print(f\"[Propensity] Restored best model with val_loss={best_val_loss:.4f}\")\n","\n","    torch.save(prop_model.state_dict(), prop_path)\n","    print(f\"[Propensity] Saved propensity model to {prop_path}\")\n","    return prop_model\n","\n","def train_tarnet_single_model(\n","    beta_norm: float,\n","    l: int,\n","    model_type: str,\n","    run_id: int = 0,         # Added for Multi-Seed Support\n","    lr: float = 1e-3,        # Added to allow custom LR per model if needed\n","    seed_split: int = 2025,\n",") -> None:\n","    \"\"\"\n","    Train a single TARNet model with robust checkpointing and regularization.\n","    \"\"\"\n","    assert model_type in [\"baseline\", \"baseline_bal\", \"propensity\", \"propensity_bal\"]\n","\n","    # Unique filename for this specific run (includes run_id)\n","    model_filename = f\"tarnet_{model_type}_beta{beta_norm}_l{l}.pt\"\n","    model_path = os.path.join(MODEL_DIR, model_filename)\n","\n","    metrics_filename = f\"metrics_{model_type}_beta{beta_norm}_l{l}.json\"\n","    metrics_path = os.path.join(RESULT_DIR, metrics_filename)\n","\n","    # if os.path.isfile(model_path) and os.path.isfile(metrics_path):\n","    #     print(f\"[SKIP] Run {run_id} already exists for {model_type}, beta={beta_norm}, l={l}\")\n","    #     return\n","\n","    print(f\"\\n[TRAIN] beta={beta_norm}, l={l}, type={model_type}, run={run_id}\")\n","\n","    # 1. Load Data\n","    X, t, y, tau = load_synthetic_dataset(beta_norm, l)\n","    N, C, H, W = X.shape\n","\n","    # 2. Deterministic Split (Unique per run_id)\n","    # Using run_id in the seed ensures different splits for different runs\n","    current_seed = seed_split + int(beta_norm * 100) + l + (run_id * 1000)\n","\n","    (X_tr, t_tr, y_tr, tau_tr), \\\n","    (X_val, t_val, y_val, tau_val), \\\n","    (X_te, t_te, y_te, tau_te) = train_val_test_split(\n","        X, t, y, tau, seed=current_seed\n","    )\n","\n","    # 3. Handle Propensity\n","    use_propensity = model_type.startswith(\"propensity\")\n","    if use_propensity:\n","        tmp_train_ds = CausalDataset(X_tr, t_tr, y_tr, tau_tr, e_hat=None)\n","        tmp_val_ds = CausalDataset(X_val, t_val, y_val, tau_val, e_hat=None)\n","\n","        prop_model = get_or_train_propensity_model(\n","            beta_norm=beta_norm,\n","            l=l,\n","            train_dataset=tmp_train_ds,\n","            val_dataset=tmp_val_ds,\n","            img_size=(H, W),\n","        ).to(device)\n","\n","        prop_model.eval()\n","        with torch.no_grad():\n","            e_tr = torch.sigmoid(prop_model(X_tr.to(device))).cpu()\n","            e_val = torch.sigmoid(prop_model(X_val.to(device))).cpu()\n","            e_te = torch.sigmoid(prop_model(X_te.to(device))).cpu()\n","\n","            # --- FIX: Clip Propensity Scores ---\n","            # Prevents instability when score is exactly 0.0 or 1.0\n","            e_tr = torch.clamp(e_tr, 0.05, 0.95)\n","            e_val = torch.clamp(e_val, 0.05, 0.95)\n","            e_te = torch.clamp(e_te, 0.05, 0.95)\n","    else:\n","        e_tr = e_val = e_te = None\n","\n","    # 4. Data Loaders\n","    train_ds = CausalDataset(X_tr, t_tr, y_tr, tau_tr, e_hat=e_tr)\n","    val_ds   = CausalDataset(X_val, t_val, y_val, tau_val, e_hat=e_val)\n","    test_ds  = CausalDataset(X_te, t_te, y_te, tau_te, e_hat=e_te)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # 5. Model Setup\n","    model = TARNet(rep_dim=64, use_propensity=use_propensity).to(device)\n","\n","    # --- FIX: Add Weight Decay (L2 Regularization) ---\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n","    mse = nn.MSELoss()\n","\n","    use_balancing = model_type.endswith(\"_bal\")\n","\n","    # 6. Training Loop with Burn-in & Early Stopping\n","    best_val_pehe = float(\"inf\")\n","    best_state = None\n","    triggers = 0\n","\n","    # Burn-in: Ignore validation metrics for first 20 epochs to avoid \"lucky\" initialization checkpoints\n","    BURN_IN_EPOCHS = 0\n","\n","    for epoch in range(N_EPOCHS_TARNET):\n","        model.train()\n","        total_train_loss = 0.0\n","        n_train = 0\n","\n","        for x, t_batch, y_batch, tau_batch, e_hat_batch in train_loader:\n","            x = x.to(device)\n","            t_batch = t_batch.to(device)\n","            y_batch = y_batch.to(device)\n","            e_hat_batch = e_hat_batch.to(device)\n","\n","            if use_propensity:\n","                y0_hat, y1_hat, phi_x = model(x, e_hat=e_hat_batch)\n","            else:\n","                y0_hat, y1_hat, phi_x = model(x, e_hat=None)\n","\n","            y_pred = torch.where(t_batch > 0.5, y1_hat, y0_hat)\n","            factual_loss = mse(y_pred, y_batch)\n","\n","            if use_balancing:\n","                bal_loss = balancing_loss(phi_x, t_batch)\n","                loss = factual_loss + LAMBDA_BAL * bal_loss\n","            else:\n","                loss = factual_loss\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_train_loss += loss.item() * x.size(0)\n","            n_train += x.size(0)\n","\n","        avg_train_loss = total_train_loss / n_train if n_train > 0 else 0.0\n","\n","        # --- Validation & Checkpointing ---\n","        val_pehe = compute_pehe(model, val_loader, use_propensity=use_propensity)\n","        val_mse_f = compute_mse_factual(model, val_loader, use_propensity=use_propensity)\n","\n","        # Only check for best model AFTER burn-in period\n","        if epoch >= BURN_IN_EPOCHS:\n","            if val_pehe < best_val_pehe:\n","                best_val_pehe = val_pehe\n","                best_state = model.state_dict()\n","                triggers = 0\n","            else:\n","                triggers += 1\n","        else:\n","            # During burn-in, reset triggers (don't early stop)\n","            triggers = 0\n","\n","        # Logging\n","        if (epoch + 1) % 50 == 0 or triggers >= PATIENCE:\n","            status = \"(Burn-in)\" if epoch < BURN_IN_EPOCHS else f\"(Best: {best_val_pehe:.4f})\"\n","            print(\n","                f\"[{model_type} run={run_id}] Ep {epoch+1} | \"\n","                f\"Loss: {avg_train_loss:.4f} | MSE: {val_mse_f:.4f} | \"\n","                f\"PEHE: {val_pehe:.4f} {status}\"\n","            )\n","\n","        if triggers >= PATIENCE:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # 7. Final Evaluation (Load Best State)\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    else:\n","        print(\"[WARNING] No best state found (did not pass burn-in?). Using last state.\")\n","\n","    train_pehe = compute_pehe(model, train_loader, use_propensity=use_propensity)\n","    val_pehe_final = compute_pehe(model, val_loader, use_propensity=use_propensity)\n","    test_pehe = compute_pehe(model, test_loader, use_propensity=use_propensity)\n","\n","    metrics = {\n","        \"beta_norm\": beta_norm,\n","        \"l\": l,\n","        \"model_type\": model_type,\n","        \"run_id\": run_id,\n","        \"train_PEHE\": train_pehe,\n","        \"val_PEHE\": val_pehe_final,\n","        \"best_val_PEHE\": best_val_pehe, # Explicitly save the tracked minimum\n","        \"test_PEHE\": test_pehe,\n","        \"epochs\": N_EPOCHS_TARNET,\n","        \"lambda_bal\": LAMBDA_BAL,\n","        \"lr\": lr\n","    }\n","\n","    torch.save(model.state_dict(), model_path)\n","    with open(metrics_path, \"w\") as f:\n","        json.dump(metrics, f, indent=2)\n","\n","    print(f\"[SAVE] Run {run_id} complete. Test PEHE: {test_pehe:.4f}\")\n","\n","\n","# ============================================================\n","# Main loops\n","# ============================================================\n","\n","if __name__ == \"__main__\":\n","    beta_list = [0.5, 1.0, 2.0, 4.0]\n","    l_list = [0, 1, 2, 4, 8]\n","    model_types = [\"baseline\", \"baseline_bal\", \"propensity\"]  #\"propensity_bal\"]\n","\n","    '''\n","    for l in l_list:\n","        train_tarnet_single_model(beta_norm=4.0, l=l, model_type=\"baseline\")\n","        train_tarnet_single_model(beta_norm=4.0, l=l, model_type=\"baseline_bal\")\n","        train_tarnet_single_model(beta_norm=4.0, l=l, model_type=\"propensity\")\n","    '''\n","\n","\n","    for beta_norm in beta_list:\n","        for l in l_list:\n","            for mtype in model_types:\n","                train_tarnet_single_model(beta_norm=beta_norm, l=l, model_type=mtype)\n",""]},{"cell_type":"code","source":[],"metadata":{"id":"2SgPO7j06cWv"},"execution_count":null,"outputs":[]}]}