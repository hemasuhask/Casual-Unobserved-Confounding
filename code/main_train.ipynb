{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPfoxl6GexaDmEaKa+C/64g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYhaiaGT6O5G","executionInfo":{"status":"ok","timestamp":1764991536105,"user_tz":300,"elapsed":19092,"user":{"displayName":"Lin Z","userId":"11285634647307012657"}},"outputId":"3ef0d43b-eabb-4d12-f2a2-9da499b3c70b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlP0O7UP6NdQ","executionInfo":{"status":"ok","timestamp":1764993890842,"user_tz":300,"elapsed":2336441,"user":{"displayName":"Lin Z","userId":"11285634647307012657"}},"outputId":"2acdfdd5-90ad-444e-c0c0-c210f1ae71d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","[TRAIN] beta=0.5, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0145 | MSE: 0.0108 | PEHE: 0.0226 (Best: 0.0046)\n","[baseline run=0] Ep 100 | Loss: 0.0132 | MSE: 0.0107 | PEHE: 0.0174 (Best: 0.0041)\n","[baseline run=0] Ep 150 | Loss: 0.0115 | MSE: 0.0106 | PEHE: 0.0061 (Best: 0.0038)\n","[baseline run=0] Ep 200 | Loss: 0.0104 | MSE: 0.0106 | PEHE: 0.0100 (Best: 0.0038)\n","[baseline run=0] Ep 250 | Loss: 0.0099 | MSE: 0.0106 | PEHE: 0.0040 (Best: 0.0038)\n","[baseline run=0] Ep 264 | Loss: 0.0099 | MSE: 0.0106 | PEHE: 0.0043 (Best: 0.0038)\n","Early stopping at epoch 264\n","[SAVE] Run 0 complete. Test PEHE: 0.0043\n","\n","[TRAIN] beta=0.5, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.8260 | MSE: 0.0106 | PEHE: 0.0100 (Best: 0.0038)\n","[baseline_bal run=0] Ep 70 | Loss: 0.7292 | MSE: 0.0107 | PEHE: 0.0162 (Best: 0.0038)\n","Early stopping at epoch 70\n","[SAVE] Run 0 complete. Test PEHE: 0.0164\n","\n","[TRAIN] beta=0.5, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=0\n","[Propensity beta=0.5, l=0] Epoch 10/50, train_loss=0.5648, val_loss=0.7038 (Best: 0.6890)\n","[Propensity beta=0.5, l=0] Epoch 20/50, train_loss=0.3207, val_loss=0.8941 (Best: 0.6890)\n","[Propensity beta=0.5, l=0] Epoch 30/50, train_loss=0.1767, val_loss=1.1679 (Best: 0.6890)\n","[Propensity beta=0.5, l=0] Epoch 40/50, train_loss=0.0871, val_loss=1.4591 (Best: 0.6890)\n","[Propensity beta=0.5, l=0] Epoch 50/50, train_loss=0.0438, val_loss=1.7231 (Best: 0.6890)\n","[Propensity] Restored best model with val_loss=0.6890\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta0.5_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0156 | MSE: 0.0144 | PEHE: 0.0833 (Best: 0.0637)\n","[propensity run=0] Ep 53 | Loss: 0.0149 | MSE: 0.0167 | PEHE: 0.1083 (Best: 0.0637)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.1076\n","\n","[TRAIN] beta=0.5, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.9743 | MSE: 0.9685 | PEHE: 0.4816 (Best: 0.1832)\n","[baseline run=0] Ep 52 | Loss: 0.9736 | MSE: 0.9687 | PEHE: 0.4007 (Best: 0.1832)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 0.4000\n","\n","[TRAIN] beta=0.5, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 1.9392 | MSE: 0.9695 | PEHE: 0.4080 (Best: 0.1668)\n","[baseline_bal run=0] Ep 52 | Loss: 1.7948 | MSE: 0.9693 | PEHE: 0.4176 (Best: 0.1668)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 0.4175\n","\n","[TRAIN] beta=0.5, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=1\n","[Propensity beta=0.5, l=1] Epoch 10/50, train_loss=0.6046, val_loss=0.6991 (Best: 0.6909)\n","[Propensity beta=0.5, l=1] Epoch 20/50, train_loss=0.3819, val_loss=0.8805 (Best: 0.6909)\n","[Propensity beta=0.5, l=1] Epoch 30/50, train_loss=0.2496, val_loss=1.1656 (Best: 0.6909)\n","[Propensity beta=0.5, l=1] Epoch 40/50, train_loss=0.1524, val_loss=1.4745 (Best: 0.6909)\n","[Propensity beta=0.5, l=1] Epoch 50/50, train_loss=0.0866, val_loss=1.7846 (Best: 0.6909)\n","[Propensity] Restored best model with val_loss=0.6909\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta0.5_l1.pt\n","[propensity run=0] Ep 50 | Loss: 0.9729 | MSE: 0.9808 | PEHE: 0.3203 (Best: 0.2717)\n","[propensity run=0] Ep 86 | Loss: 0.9716 | MSE: 0.9769 | PEHE: 0.3539 (Best: 0.2717)\n","Early stopping at epoch 86\n","[SAVE] Run 0 complete. Test PEHE: 0.3569\n","\n","[TRAIN] beta=0.5, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 1.8047 | MSE: 1.8570 | PEHE: 0.9090 (Best: 0.5514)\n","[baseline run=0] Ep 52 | Loss: 1.8043 | MSE: 1.8592 | PEHE: 0.7814 (Best: 0.5514)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 0.7815\n","\n","[TRAIN] beta=0.5, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 2.6280 | MSE: 1.8578 | PEHE: 0.8117 (Best: 0.1804)\n","[baseline_bal run=0] Ep 52 | Loss: 2.5147 | MSE: 1.8568 | PEHE: 0.8361 (Best: 0.1804)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 0.8363\n","\n","[TRAIN] beta=0.5, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=2\n","[Propensity beta=0.5, l=2] Epoch 10/50, train_loss=0.5634, val_loss=0.7105 (Best: 0.6898)\n","[Propensity beta=0.5, l=2] Epoch 20/50, train_loss=0.3384, val_loss=0.9136 (Best: 0.6898)\n","[Propensity beta=0.5, l=2] Epoch 30/50, train_loss=0.1971, val_loss=1.1856 (Best: 0.6898)\n","[Propensity beta=0.5, l=2] Epoch 40/50, train_loss=0.1033, val_loss=1.4912 (Best: 0.6898)\n","[Propensity beta=0.5, l=2] Epoch 50/50, train_loss=0.0532, val_loss=1.7808 (Best: 0.6898)\n","[Propensity] Restored best model with val_loss=0.6898\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta0.5_l2.pt\n","[propensity run=0] Ep 50 | Loss: 1.8022 | MSE: 1.8617 | PEHE: 0.7046 (Best: 0.5556)\n","[propensity run=0] Ep 100 | Loss: 1.8031 | MSE: 1.8683 | PEHE: 0.6518 (Best: 0.5392)\n","[propensity run=0] Ep 106 | Loss: 1.7957 | MSE: 1.8711 | PEHE: 0.6288 (Best: 0.5392)\n","Early stopping at epoch 106\n","[SAVE] Run 0 complete. Test PEHE: 0.6293\n","\n","[TRAIN] beta=0.5, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 3.4147 | MSE: 3.3502 | PEHE: 1.4622 (Best: 1.1782)\n","[baseline run=0] Ep 52 | Loss: 3.4070 | MSE: 3.3527 | PEHE: 1.6287 (Best: 1.1782)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 1.6286\n","\n","[TRAIN] beta=0.5, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 3.9992 | MSE: 3.3501 | PEHE: 1.5848 (Best: 0.3446)\n","[baseline_bal run=0] Ep 52 | Loss: 4.0819 | MSE: 3.3501 | PEHE: 1.5870 (Best: 0.3446)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 1.5870\n","\n","[TRAIN] beta=0.5, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=4\n","[Propensity beta=0.5, l=4] Epoch 10/50, train_loss=0.5847, val_loss=0.6885 (Best: 0.6849)\n","[Propensity beta=0.5, l=4] Epoch 20/50, train_loss=0.3531, val_loss=0.8703 (Best: 0.6849)\n","[Propensity beta=0.5, l=4] Epoch 30/50, train_loss=0.2315, val_loss=1.1512 (Best: 0.6849)\n","[Propensity beta=0.5, l=4] Epoch 40/50, train_loss=0.1457, val_loss=1.4370 (Best: 0.6849)\n","[Propensity beta=0.5, l=4] Epoch 50/50, train_loss=0.0848, val_loss=1.7360 (Best: 0.6849)\n","[Propensity] Restored best model with val_loss=0.6849\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta0.5_l4.pt\n","[propensity run=0] Ep 50 | Loss: 3.4031 | MSE: 3.3201 | PEHE: 1.1377 (Best: 1.0957)\n","[propensity run=0] Ep 100 | Loss: 3.3786 | MSE: 3.3731 | PEHE: 0.9754 (Best: 0.8535)\n","[propensity run=0] Ep 150 | Loss: 3.3692 | MSE: 3.3925 | PEHE: 0.8900 (Best: 0.8036)\n","[propensity run=0] Ep 171 | Loss: 3.3606 | MSE: 3.3838 | PEHE: 0.9060 (Best: 0.8036)\n","Early stopping at epoch 171\n","[SAVE] Run 0 complete. Test PEHE: 0.9022\n","\n","[TRAIN] beta=0.5, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 6.1266 | MSE: 5.8069 | PEHE: 2.5816 (Best: 2.2641)\n","[baseline run=0] Ep 52 | Loss: 6.1383 | MSE: 5.8101 | PEHE: 2.5894 (Best: 2.2641)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 2.5909\n","\n","[TRAIN] beta=0.5, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 6.1611 | MSE: 5.8038 | PEHE: 2.6623 (Best: 0.4846)\n","[baseline_bal run=0] Ep 52 | Loss: 6.1375 | MSE: 5.7998 | PEHE: 2.7510 (Best: 0.4846)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 2.7511\n","\n","[TRAIN] beta=0.5, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=0.5, l=8\n","[Propensity beta=0.5, l=8] Epoch 10/50, train_loss=0.5928, val_loss=0.6982 (Best: 0.6914)\n","[Propensity beta=0.5, l=8] Epoch 20/50, train_loss=0.3640, val_loss=0.8610 (Best: 0.6914)\n","[Propensity beta=0.5, l=8] Epoch 30/50, train_loss=0.2338, val_loss=1.1097 (Best: 0.6914)\n","[Propensity beta=0.5, l=8] Epoch 40/50, train_loss=0.1391, val_loss=1.3898 (Best: 0.6914)\n","[Propensity beta=0.5, l=8] Epoch 50/50, train_loss=0.0767, val_loss=1.6582 (Best: 0.6914)\n","[Propensity] Restored best model with val_loss=0.6914\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta0.5_l8.pt\n","[propensity run=0] Ep 50 | Loss: 6.1819 | MSE: 5.8186 | PEHE: 2.5123 (Best: 2.2860)\n","[propensity run=0] Ep 52 | Loss: 6.1829 | MSE: 5.8542 | PEHE: 2.3576 (Best: 2.2860)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 2.3612\n","\n","[TRAIN] beta=1.0, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0183 | MSE: 0.0103 | PEHE: 0.0294 (Best: 0.0048)\n","[baseline run=0] Ep 100 | Loss: 0.0147 | MSE: 0.0100 | PEHE: 0.0092 (Best: 0.0037)\n","[baseline run=0] Ep 150 | Loss: 0.0126 | MSE: 0.0100 | PEHE: 0.0039 (Best: 0.0037)\n","[baseline run=0] Ep 189 | Loss: 0.0113 | MSE: 0.0101 | PEHE: 0.0201 (Best: 0.0037)\n","Early stopping at epoch 189\n","[SAVE] Run 0 complete. Test PEHE: 0.0202\n","\n","[TRAIN] beta=1.0, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.7423 | MSE: 0.0100 | PEHE: 0.0133 (Best: 0.0038)\n","[baseline_bal run=0] Ep 100 | Loss: 0.5237 | MSE: 0.0101 | PEHE: 0.0217 (Best: 0.0037)\n","[baseline_bal run=0] Ep 150 | Loss: 0.6869 | MSE: 0.0100 | PEHE: 0.0039 (Best: 0.0037)\n","[baseline_bal run=0] Ep 200 | Loss: 0.6899 | MSE: 0.0100 | PEHE: 0.0084 (Best: 0.0036)\n","[baseline_bal run=0] Ep 232 | Loss: 0.7152 | MSE: 0.0100 | PEHE: 0.0053 (Best: 0.0036)\n","Early stopping at epoch 232\n","[SAVE] Run 0 complete. Test PEHE: 0.0053\n","\n","[TRAIN] beta=1.0, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=0\n","[Propensity beta=1.0, l=0] Epoch 10/50, train_loss=0.5775, val_loss=0.7130 (Best: 0.6894)\n","[Propensity beta=1.0, l=0] Epoch 20/50, train_loss=0.3437, val_loss=0.9331 (Best: 0.6894)\n","[Propensity beta=1.0, l=0] Epoch 30/50, train_loss=0.1969, val_loss=1.2597 (Best: 0.6894)\n","[Propensity beta=1.0, l=0] Epoch 40/50, train_loss=0.0990, val_loss=1.6260 (Best: 0.6894)\n","[Propensity beta=1.0, l=0] Epoch 50/50, train_loss=0.0502, val_loss=1.9494 (Best: 0.6894)\n","[Propensity] Restored best model with val_loss=0.6894\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta1.0_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0154 | MSE: 0.0154 | PEHE: 0.1054 (Best: 0.0536)\n","[propensity run=0] Ep 53 | Loss: 0.0152 | MSE: 0.0139 | PEHE: 0.0873 (Best: 0.0536)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.0858\n","\n","[TRAIN] beta=1.0, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 3.4612 | MSE: 3.4480 | PEHE: 1.5337 (Best: 1.3075)\n","[baseline run=0] Ep 52 | Loss: 3.4505 | MSE: 3.4487 | PEHE: 1.5304 (Best: 1.3075)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 1.5302\n","\n","[TRAIN] beta=1.0, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 3.4477 | MSE: 3.4493 | PEHE: 1.5327 (Best: 0.2930)\n","[baseline_bal run=0] Ep 52 | Loss: 3.4566 | MSE: 3.4494 | PEHE: 1.4688 (Best: 0.2930)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 1.4687\n","\n","[TRAIN] beta=1.0, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=1\n","[Propensity beta=1.0, l=1] Epoch 10/50, train_loss=0.5972, val_loss=0.7065 (Best: 0.6895)\n","[Propensity beta=1.0, l=1] Epoch 20/50, train_loss=0.3754, val_loss=0.8495 (Best: 0.6895)\n","[Propensity beta=1.0, l=1] Epoch 30/50, train_loss=0.2443, val_loss=1.0590 (Best: 0.6895)\n","[Propensity beta=1.0, l=1] Epoch 40/50, train_loss=0.1429, val_loss=1.3398 (Best: 0.6895)\n","[Propensity beta=1.0, l=1] Epoch 50/50, train_loss=0.0776, val_loss=1.6296 (Best: 0.6895)\n","[Propensity] Restored best model with val_loss=0.6895\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta1.0_l1.pt\n","[propensity run=0] Ep 50 | Loss: 3.4196 | MSE: 3.4490 | PEHE: 1.1516 (Best: 0.9323)\n","[propensity run=0] Ep 88 | Loss: 3.4020 | MSE: 3.4668 | PEHE: 1.0689 (Best: 0.9323)\n","Early stopping at epoch 88\n","[SAVE] Run 0 complete. Test PEHE: 1.0751\n","\n","[TRAIN] beta=1.0, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 6.0316 | MSE: 5.9300 | PEHE: 2.9875 (Best: 2.4225)\n","[baseline run=0] Ep 92 | Loss: 5.9946 | MSE: 5.9091 | PEHE: 3.0421 (Best: 2.4225)\n","Early stopping at epoch 92\n","[SAVE] Run 0 complete. Test PEHE: 3.0344\n","\n","[TRAIN] beta=1.0, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 6.7982 | MSE: 5.9280 | PEHE: 2.8038 (Best: 0.4418)\n","[baseline_bal run=0] Ep 52 | Loss: 6.7771 | MSE: 5.9278 | PEHE: 2.8533 (Best: 0.4418)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 2.8531\n","\n","[TRAIN] beta=1.0, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=2\n","[Propensity beta=1.0, l=2] Epoch 10/50, train_loss=0.5558, val_loss=0.6994 (Best: 0.6878)\n","[Propensity beta=1.0, l=2] Epoch 20/50, train_loss=0.3333, val_loss=0.9250 (Best: 0.6878)\n","[Propensity beta=1.0, l=2] Epoch 30/50, train_loss=0.2150, val_loss=1.2039 (Best: 0.6878)\n","[Propensity beta=1.0, l=2] Epoch 40/50, train_loss=0.1282, val_loss=1.5341 (Best: 0.6878)\n","[Propensity beta=1.0, l=2] Epoch 50/50, train_loss=0.0718, val_loss=1.8622 (Best: 0.6878)\n","[Propensity] Restored best model with val_loss=0.6878\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta1.0_l2.pt\n","[propensity run=0] Ep 50 | Loss: 6.0028 | MSE: 5.9323 | PEHE: 2.4092 (Best: 2.2322)\n","[propensity run=0] Ep 100 | Loss: 5.9710 | MSE: 6.1197 | PEHE: 2.0602 (Best: 1.8008)\n","[propensity run=0] Ep 150 | Loss: 5.8222 | MSE: 6.0324 | PEHE: 1.9458 (Best: 1.6801)\n","[propensity run=0] Ep 160 | Loss: 5.7921 | MSE: 6.0031 | PEHE: 1.9822 (Best: 1.6801)\n","Early stopping at epoch 160\n","[SAVE] Run 0 complete. Test PEHE: 1.9551\n","\n","[TRAIN] beta=1.0, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 10.5159 | MSE: 9.9397 | PEHE: 4.3293 (Best: 3.9220)\n","[baseline run=0] Ep 60 | Loss: 10.5311 | MSE: 9.8769 | PEHE: 4.5414 (Best: 3.9220)\n","Early stopping at epoch 60\n","[SAVE] Run 0 complete. Test PEHE: 4.5421\n","\n","[TRAIN] beta=1.0, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 11.1742 | MSE: 9.8757 | PEHE: 4.5938 (Best: 0.2875)\n","[baseline_bal run=0] Ep 52 | Loss: 11.1890 | MSE: 9.8584 | PEHE: 4.6887 (Best: 0.2875)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 4.6890\n","\n","[TRAIN] beta=1.0, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=4\n","[Propensity beta=1.0, l=4] Epoch 10/50, train_loss=0.5984, val_loss=0.6761 (Best: 0.6761)\n","[Propensity beta=1.0, l=4] Epoch 20/50, train_loss=0.3654, val_loss=0.7960 (Best: 0.6761)\n","[Propensity beta=1.0, l=4] Epoch 30/50, train_loss=0.2500, val_loss=1.0209 (Best: 0.6761)\n","[Propensity beta=1.0, l=4] Epoch 40/50, train_loss=0.1644, val_loss=1.2784 (Best: 0.6761)\n","[Propensity beta=1.0, l=4] Epoch 50/50, train_loss=0.1000, val_loss=1.5639 (Best: 0.6761)\n","[Propensity] Restored best model with val_loss=0.6761\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta1.0_l4.pt\n","[propensity run=0] Ep 50 | Loss: 10.4336 | MSE: 10.1605 | PEHE: 3.5773 (Best: 3.4042)\n","[propensity run=0] Ep 100 | Loss: 10.1545 | MSE: 10.6326 | PEHE: 3.2803 (Best: 2.7834)\n","[propensity run=0] Ep 146 | Loss: 9.9003 | MSE: 10.5517 | PEHE: 3.1653 (Best: 2.7834)\n","Early stopping at epoch 146\n","[SAVE] Run 0 complete. Test PEHE: 3.1664\n","\n","[TRAIN] beta=1.0, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 17.7697 | MSE: 17.3053 | PEHE: 7.4821 (Best: 6.5219)\n","[baseline run=0] Ep 53 | Loss: 17.7660 | MSE: 17.2796 | PEHE: 7.4834 (Best: 6.5219)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 7.4846\n","\n","[TRAIN] beta=1.0, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 17.7476 | MSE: 17.3309 | PEHE: 7.2547 (Best: 0.6386)\n","[baseline_bal run=0] Ep 52 | Loss: 17.7876 | MSE: 17.2942 | PEHE: 7.4774 (Best: 0.6386)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 7.4775\n","\n","[TRAIN] beta=1.0, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=1.0, l=8\n","[Propensity beta=1.0, l=8] Epoch 10/50, train_loss=0.5770, val_loss=0.7029 (Best: 0.6928)\n","[Propensity beta=1.0, l=8] Epoch 20/50, train_loss=0.3522, val_loss=0.8937 (Best: 0.6928)\n","[Propensity beta=1.0, l=8] Epoch 30/50, train_loss=0.2271, val_loss=1.1722 (Best: 0.6928)\n","[Propensity beta=1.0, l=8] Epoch 40/50, train_loss=0.1342, val_loss=1.4778 (Best: 0.6928)\n","[Propensity beta=1.0, l=8] Epoch 50/50, train_loss=0.0737, val_loss=1.7880 (Best: 0.6928)\n","[Propensity] Restored best model with val_loss=0.6928\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta1.0_l8.pt\n","[propensity run=0] Ep 50 | Loss: 17.6831 | MSE: 17.4054 | PEHE: 6.9680 (Best: 6.4396)\n","[propensity run=0] Ep 100 | Loss: 17.7422 | MSE: 17.5497 | PEHE: 6.7408 (Best: 5.9859)\n","[propensity run=0] Ep 139 | Loss: 17.6089 | MSE: 17.7528 | PEHE: 6.5457 (Best: 5.9859)\n","Early stopping at epoch 139\n","[SAVE] Run 0 complete. Test PEHE: 6.5229\n","\n","[TRAIN] beta=2.0, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0150 | MSE: 0.0103 | PEHE: 0.0153 (Best: 0.0045)\n","[baseline run=0] Ep 100 | Loss: 0.0128 | MSE: 0.0102 | PEHE: 0.0058 (Best: 0.0039)\n","[baseline run=0] Ep 128 | Loss: 0.0117 | MSE: 0.0102 | PEHE: 0.0065 (Best: 0.0039)\n","Early stopping at epoch 128\n","[SAVE] Run 0 complete. Test PEHE: 0.0063\n","\n","[TRAIN] beta=2.0, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.6652 | MSE: 0.0103 | PEHE: 0.0086 (Best: 0.0038)\n","[baseline_bal run=0] Ep 72 | Loss: 0.7787 | MSE: 0.0105 | PEHE: 0.0237 (Best: 0.0038)\n","Early stopping at epoch 72\n","[SAVE] Run 0 complete. Test PEHE: 0.0235\n","\n","[TRAIN] beta=2.0, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=0\n","[Propensity beta=2.0, l=0] Epoch 10/50, train_loss=0.5873, val_loss=0.7054 (Best: 0.6912)\n","[Propensity beta=2.0, l=0] Epoch 20/50, train_loss=0.3415, val_loss=0.9077 (Best: 0.6912)\n","[Propensity beta=2.0, l=0] Epoch 30/50, train_loss=0.1989, val_loss=1.1958 (Best: 0.6912)\n","[Propensity beta=2.0, l=0] Epoch 40/50, train_loss=0.1023, val_loss=1.4986 (Best: 0.6912)\n","[Propensity beta=2.0, l=0] Epoch 50/50, train_loss=0.0506, val_loss=1.7840 (Best: 0.6912)\n","[Propensity] Restored best model with val_loss=0.6912\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta2.0_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0185 | MSE: 0.0161 | PEHE: 0.1042 (Best: 0.0392)\n","[propensity run=0] Ep 53 | Loss: 0.0179 | MSE: 0.0165 | PEHE: 0.1060 (Best: 0.0392)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 0.1059\n","\n","[TRAIN] beta=2.0, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 10.8250 | MSE: 10.5949 | PEHE: 4.2742 (Best: 3.9491)\n","[baseline run=0] Ep 53 | Loss: 10.7583 | MSE: 10.5292 | PEHE: 4.9192 (Best: 3.9491)\n","Early stopping at epoch 53\n","[SAVE] Run 0 complete. Test PEHE: 4.9189\n","\n","[TRAIN] beta=2.0, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 11.5013 | MSE: 10.5534 | PEHE: 4.4635 (Best: 0.1942)\n","[baseline_bal run=0] Ep 52 | Loss: 11.5928 | MSE: 10.5469 | PEHE: 4.5045 (Best: 0.1942)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 4.5043\n","\n","[TRAIN] beta=2.0, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=1\n","[Propensity beta=2.0, l=1] Epoch 10/50, train_loss=0.5805, val_loss=0.6792 (Best: 0.6780)\n","[Propensity beta=2.0, l=1] Epoch 20/50, train_loss=0.3620, val_loss=0.8297 (Best: 0.6780)\n","[Propensity beta=2.0, l=1] Epoch 30/50, train_loss=0.2471, val_loss=1.0634 (Best: 0.6780)\n","[Propensity beta=2.0, l=1] Epoch 40/50, train_loss=0.1671, val_loss=1.3141 (Best: 0.6780)\n","[Propensity beta=2.0, l=1] Epoch 50/50, train_loss=0.1013, val_loss=1.5829 (Best: 0.6780)\n","[Propensity] Restored best model with val_loss=0.6780\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta2.0_l1.pt\n","[propensity run=0] Ep 50 | Loss: 10.6994 | MSE: 10.9843 | PEHE: 3.4249 (Best: 3.2962)\n","[propensity run=0] Ep 100 | Loss: 10.3101 | MSE: 11.0841 | PEHE: 3.6866 (Best: 2.6446)\n","[propensity run=0] Ep 143 | Loss: 9.9646 | MSE: 11.2597 | PEHE: 3.1655 (Best: 2.6446)\n","Early stopping at epoch 143\n","[SAVE] Run 0 complete. Test PEHE: 3.1447\n","\n","[TRAIN] beta=2.0, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 18.3733 | MSE: 16.6477 | PEHE: 7.0160 (Best: 6.7716)\n","[baseline run=0] Ep 71 | Loss: 18.2336 | MSE: 16.5198 | PEHE: 7.4017 (Best: 6.7716)\n","Early stopping at epoch 71\n","[SAVE] Run 0 complete. Test PEHE: 7.3999\n","\n","[TRAIN] beta=2.0, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 19.3811 | MSE: 16.5635 | PEHE: 7.5050 (Best: 0.0902)\n","[baseline_bal run=0] Ep 52 | Loss: 19.2012 | MSE: 16.5792 | PEHE: 7.5460 (Best: 0.0902)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 7.5460\n","\n","[TRAIN] beta=2.0, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=2\n","[Propensity beta=2.0, l=2] Epoch 10/50, train_loss=0.5793, val_loss=0.6711 (Best: 0.6711)\n","[Propensity beta=2.0, l=2] Epoch 20/50, train_loss=0.3587, val_loss=0.7997 (Best: 0.6711)\n","[Propensity beta=2.0, l=2] Epoch 30/50, train_loss=0.2317, val_loss=1.0404 (Best: 0.6711)\n","[Propensity beta=2.0, l=2] Epoch 40/50, train_loss=0.1402, val_loss=1.3197 (Best: 0.6711)\n","[Propensity beta=2.0, l=2] Epoch 50/50, train_loss=0.0804, val_loss=1.5896 (Best: 0.6711)\n","[Propensity] Restored best model with val_loss=0.6711\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta2.0_l2.pt\n","[propensity run=0] Ep 50 | Loss: 18.0814 | MSE: 16.3506 | PEHE: 7.0314 (Best: 6.0677)\n","[propensity run=0] Ep 52 | Loss: 18.2881 | MSE: 16.7034 | PEHE: 6.2706 (Best: 6.0677)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 6.2594\n","\n","[TRAIN] beta=2.0, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 30.8618 | MSE: 32.5378 | PEHE: 10.8250 (Best: 10.3518)\n","[baseline run=0] Ep 52 | Loss: 30.5890 | MSE: 32.5106 | PEHE: 10.8522 (Best: 10.3518)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 10.8525\n","\n","[TRAIN] beta=2.0, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 31.4835 | MSE: 32.6506 | PEHE: 11.4986 (Best: 0.5127)\n","[baseline_bal run=0] Ep 52 | Loss: 31.8720 | MSE: 32.6619 | PEHE: 11.4554 (Best: 0.5127)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 11.4553\n","\n","[TRAIN] beta=2.0, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=4\n","[Propensity beta=2.0, l=4] Epoch 10/50, train_loss=0.5499, val_loss=0.6528 (Best: 0.6528)\n","[Propensity beta=2.0, l=4] Epoch 20/50, train_loss=0.3181, val_loss=0.7755 (Best: 0.6514)\n","[Propensity beta=2.0, l=4] Epoch 30/50, train_loss=0.2136, val_loss=0.9989 (Best: 0.6514)\n","[Propensity beta=2.0, l=4] Epoch 40/50, train_loss=0.1393, val_loss=1.2764 (Best: 0.6514)\n","[Propensity beta=2.0, l=4] Epoch 50/50, train_loss=0.0866, val_loss=1.5716 (Best: 0.6514)\n","[Propensity] Restored best model with val_loss=0.6514\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta2.0_l4.pt\n","[propensity run=0] Ep 50 | Loss: 31.0248 | MSE: 32.5176 | PEHE: 10.0778 (Best: 10.0778)\n","[propensity run=0] Ep 100 | Loss: 29.8529 | MSE: 31.7039 | PEHE: 10.3143 (Best: 8.9164)\n","[propensity run=0] Ep 150 | Loss: 29.5322 | MSE: 33.3081 | PEHE: 9.1302 (Best: 7.2247)\n","[propensity run=0] Ep 200 | Loss: 29.3475 | MSE: 36.1075 | PEHE: 7.6870 (Best: 7.1981)\n","[propensity run=0] Ep 217 | Loss: 28.9895 | MSE: 36.2870 | PEHE: 7.6923 (Best: 7.1981)\n","Early stopping at epoch 217\n","[SAVE] Run 0 complete. Test PEHE: 7.6692\n","\n","[TRAIN] beta=2.0, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 58.5067 | MSE: 54.8645 | PEHE: 15.4622 (Best: 14.7608)\n","[baseline run=0] Ep 52 | Loss: 58.6241 | MSE: 54.4100 | PEHE: 17.2590 (Best: 14.7608)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 17.2517\n","\n","[TRAIN] beta=2.0, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 59.7362 | MSE: 54.5248 | PEHE: 16.3529 (Best: 0.7037)\n","[baseline_bal run=0] Ep 52 | Loss: 59.1664 | MSE: 54.4790 | PEHE: 16.5093 (Best: 0.7037)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 16.5093\n","\n","[TRAIN] beta=2.0, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=2.0, l=8\n","[Propensity beta=2.0, l=8] Epoch 10/50, train_loss=0.5511, val_loss=0.6965 (Best: 0.6868)\n","[Propensity beta=2.0, l=8] Epoch 20/50, train_loss=0.3222, val_loss=0.8908 (Best: 0.6868)\n","[Propensity beta=2.0, l=8] Epoch 30/50, train_loss=0.2016, val_loss=1.1393 (Best: 0.6868)\n","[Propensity beta=2.0, l=8] Epoch 40/50, train_loss=0.1096, val_loss=1.4460 (Best: 0.6868)\n","[Propensity beta=2.0, l=8] Epoch 50/50, train_loss=0.0553, val_loss=1.7513 (Best: 0.6868)\n","[Propensity] Restored best model with val_loss=0.6868\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta2.0_l8.pt\n","[propensity run=0] Ep 50 | Loss: 59.2412 | MSE: 54.4685 | PEHE: 17.2751 (Best: 15.1297)\n","[propensity run=0] Ep 66 | Loss: 58.8511 | MSE: 54.3855 | PEHE: 16.3679 (Best: 15.1297)\n","Early stopping at epoch 66\n","[SAVE] Run 0 complete. Test PEHE: 16.3681\n","\n","[TRAIN] beta=4.0, l=0, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 0.0177 | MSE: 0.0097 | PEHE: 0.0164 (Best: 0.0048)\n","[baseline run=0] Ep 100 | Loss: 0.0147 | MSE: 0.0096 | PEHE: 0.0125 (Best: 0.0043)\n","[baseline run=0] Ep 114 | Loss: 0.0138 | MSE: 0.0096 | PEHE: 0.0099 (Best: 0.0043)\n","Early stopping at epoch 114\n","[SAVE] Run 0 complete. Test PEHE: 0.0099\n","\n","[TRAIN] beta=4.0, l=0, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 0.6484 | MSE: 0.0098 | PEHE: 0.0273 (Best: 0.0038)\n","[baseline_bal run=0] Ep 81 | Loss: 0.4822 | MSE: 0.0096 | PEHE: 0.0139 (Best: 0.0038)\n","Early stopping at epoch 81\n","[SAVE] Run 0 complete. Test PEHE: 0.0138\n","\n","[TRAIN] beta=4.0, l=0, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=0\n","[Propensity beta=4.0, l=0] Epoch 10/50, train_loss=0.6153, val_loss=0.6961 (Best: 0.6911)\n","[Propensity beta=4.0, l=0] Epoch 20/50, train_loss=0.3738, val_loss=0.8599 (Best: 0.6911)\n","[Propensity beta=4.0, l=0] Epoch 30/50, train_loss=0.2287, val_loss=1.1172 (Best: 0.6911)\n","[Propensity beta=4.0, l=0] Epoch 40/50, train_loss=0.1269, val_loss=1.4101 (Best: 0.6911)\n","[Propensity beta=4.0, l=0] Epoch 50/50, train_loss=0.0657, val_loss=1.6956 (Best: 0.6911)\n","[Propensity] Restored best model with val_loss=0.6911\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta4.0_l0.pt\n","[propensity run=0] Ep 50 | Loss: 0.0176 | MSE: 0.0139 | PEHE: 0.0944 (Best: 0.0484)\n","[propensity run=0] Ep 74 | Loss: 0.0163 | MSE: 0.0124 | PEHE: 0.0760 (Best: 0.0484)\n","Early stopping at epoch 74\n","[SAVE] Run 0 complete. Test PEHE: 0.0747\n","\n","[TRAIN] beta=4.0, l=1, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 31.5915 | MSE: 31.3356 | PEHE: 11.5157 (Best: 10.1812)\n","[baseline run=0] Ep 71 | Loss: 31.4980 | MSE: 31.3292 | PEHE: 11.4525 (Best: 10.1812)\n","Early stopping at epoch 71\n","[SAVE] Run 0 complete. Test PEHE: 11.4462\n","\n","[TRAIN] beta=4.0, l=1, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 32.1801 | MSE: 31.3571 | PEHE: 11.6228 (Best: 0.5172)\n","[baseline_bal run=0] Ep 52 | Loss: 32.2929 | MSE: 31.4073 | PEHE: 11.3816 (Best: 0.5172)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 11.3816\n","\n","[TRAIN] beta=4.0, l=1, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=1\n","[Propensity beta=4.0, l=1] Epoch 10/50, train_loss=0.6085, val_loss=0.6608 (Best: 0.6608)\n","[Propensity beta=4.0, l=1] Epoch 20/50, train_loss=0.3698, val_loss=0.7668 (Best: 0.6591)\n","[Propensity beta=4.0, l=1] Epoch 30/50, train_loss=0.2446, val_loss=1.0000 (Best: 0.6591)\n","[Propensity beta=4.0, l=1] Epoch 40/50, train_loss=0.1551, val_loss=1.2818 (Best: 0.6591)\n","[Propensity beta=4.0, l=1] Epoch 50/50, train_loss=0.0888, val_loss=1.5643 (Best: 0.6591)\n","[Propensity] Restored best model with val_loss=0.6591\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta4.0_l1.pt\n","[propensity run=0] Ep 50 | Loss: 31.4405 | MSE: 31.1185 | PEHE: 11.5824 (Best: 9.7224)\n","[propensity run=0] Ep 100 | Loss: 30.7370 | MSE: 34.4480 | PEHE: 8.8303 (Best: 7.9486)\n","[propensity run=0] Ep 150 | Loss: 30.9046 | MSE: 38.2689 | PEHE: 7.8258 (Best: 6.8080)\n","[propensity run=0] Ep 186 | Loss: 30.9102 | MSE: 38.7151 | PEHE: 7.1275 (Best: 6.8080)\n","Early stopping at epoch 186\n","[SAVE] Run 0 complete. Test PEHE: 7.2079\n","\n","[TRAIN] beta=4.0, l=2, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 54.6975 | MSE: 55.7073 | PEHE: 16.2222 (Best: 15.8953)\n","[baseline run=0] Ep 69 | Loss: 55.0050 | MSE: 55.8921 | PEHE: 15.9541 (Best: 15.8953)\n","Early stopping at epoch 69\n","[SAVE] Run 0 complete. Test PEHE: 15.9521\n","\n","[TRAIN] beta=4.0, l=2, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 54.3586 | MSE: 55.1536 | PEHE: 17.0393 (Best: 1.5348)\n","[baseline_bal run=0] Ep 52 | Loss: 54.1772 | MSE: 55.2635 | PEHE: 16.8250 (Best: 1.5348)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 16.8251\n","\n","[TRAIN] beta=4.0, l=2, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=2\n","[Propensity beta=4.0, l=2] Epoch 10/50, train_loss=0.6153, val_loss=0.6663 (Best: 0.6663)\n","[Propensity beta=4.0, l=2] Epoch 20/50, train_loss=0.3694, val_loss=0.7344 (Best: 0.6568)\n","[Propensity beta=4.0, l=2] Epoch 30/50, train_loss=0.2458, val_loss=0.9662 (Best: 0.6568)\n","[Propensity beta=4.0, l=2] Epoch 40/50, train_loss=0.1592, val_loss=1.2207 (Best: 0.6568)\n","[Propensity beta=4.0, l=2] Epoch 50/50, train_loss=0.0921, val_loss=1.5185 (Best: 0.6568)\n","[Propensity] Restored best model with val_loss=0.6568\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta4.0_l2.pt\n","[propensity run=0] Ep 50 | Loss: 54.0255 | MSE: 54.8446 | PEHE: 16.8387 (Best: 15.2419)\n","[propensity run=0] Ep 100 | Loss: 54.1133 | MSE: 55.8320 | PEHE: 15.3868 (Best: 13.7472)\n","[propensity run=0] Ep 150 | Loss: 53.6323 | MSE: 56.4877 | PEHE: 14.9191 (Best: 13.4524)\n","[propensity run=0] Ep 200 | Loss: 52.7118 | MSE: 59.3195 | PEHE: 13.9033 (Best: 12.4910)\n","[propensity run=0] Ep 250 | Loss: 52.7702 | MSE: 69.9467 | PEHE: 10.8204 (Best: 10.8204)\n","[propensity run=0] Ep 300 | Loss: 50.4204 | MSE: 64.6543 | PEHE: 12.8176 (Best: 10.6250)\n","[propensity run=0] Ep 322 | Loss: 49.7937 | MSE: 69.0560 | PEHE: 11.4617 (Best: 10.6250)\n","Early stopping at epoch 322\n","[SAVE] Run 0 complete. Test PEHE: 11.5375\n","\n","[TRAIN] beta=4.0, l=4, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 106.6502 | MSE: 92.7214 | PEHE: 24.6890 (Best: 17.6907)\n","[baseline run=0] Ep 52 | Loss: 106.4638 | MSE: 92.9575 | PEHE: 25.6430 (Best: 17.6907)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 25.6619\n","\n","[TRAIN] beta=4.0, l=4, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 107.2172 | MSE: 93.2329 | PEHE: 23.9521 (Best: 1.0345)\n","[baseline_bal run=0] Ep 52 | Loss: 107.4211 | MSE: 93.5679 | PEHE: 25.7893 (Best: 1.0345)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 25.7891\n","\n","[TRAIN] beta=4.0, l=4, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=4\n","[Propensity beta=4.0, l=4] Epoch 10/50, train_loss=0.5385, val_loss=0.6406 (Best: 0.6406)\n","[Propensity beta=4.0, l=4] Epoch 20/50, train_loss=0.3171, val_loss=0.7234 (Best: 0.6379)\n","[Propensity beta=4.0, l=4] Epoch 30/50, train_loss=0.2104, val_loss=0.9122 (Best: 0.6379)\n","[Propensity beta=4.0, l=4] Epoch 40/50, train_loss=0.1314, val_loss=1.1410 (Best: 0.6379)\n","[Propensity beta=4.0, l=4] Epoch 50/50, train_loss=0.0785, val_loss=1.3952 (Best: 0.6379)\n","[Propensity] Restored best model with val_loss=0.6379\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta4.0_l4.pt\n","[propensity run=0] Ep 50 | Loss: 105.9390 | MSE: 91.9915 | PEHE: 24.6479 (Best: 21.8176)\n","[propensity run=0] Ep 100 | Loss: 103.6887 | MSE: 91.0607 | PEHE: 22.4114 (Best: 20.5145)\n","[propensity run=0] Ep 150 | Loss: 102.0501 | MSE: 93.9080 | PEHE: 20.6033 (Best: 18.6816)\n","[propensity run=0] Ep 200 | Loss: 101.1520 | MSE: 105.2259 | PEHE: 17.4949 (Best: 16.6255)\n","[propensity run=0] Ep 215 | Loss: 99.3959 | MSE: 101.6390 | PEHE: 19.1529 (Best: 16.6255)\n","Early stopping at epoch 215\n","[SAVE] Run 0 complete. Test PEHE: 19.1561\n","\n","[TRAIN] beta=4.0, l=8, type=baseline, run=0\n","[baseline run=0] Ep 50 | Loss: 208.5798 | MSE: 216.0360 | PEHE: 33.6952 (Best: 23.0286)\n","[baseline run=0] Ep 52 | Loss: 209.4053 | MSE: 215.0449 | PEHE: 34.9292 (Best: 23.0286)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 34.9318\n","\n","[TRAIN] beta=4.0, l=8, type=baseline_bal, run=0\n","[baseline_bal run=0] Ep 50 | Loss: 209.3024 | MSE: 215.2451 | PEHE: 34.4355 (Best: 1.7278)\n","[baseline_bal run=0] Ep 52 | Loss: 211.3212 | MSE: 215.2543 | PEHE: 34.4103 (Best: 1.7278)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 34.4105\n","\n","[TRAIN] beta=4.0, l=8, type=propensity, run=0\n","[Propensity] Training new propensity model for beta=4.0, l=8\n","[Propensity beta=4.0, l=8] Epoch 10/50, train_loss=0.5666, val_loss=0.6880 (Best: 0.6824)\n","[Propensity beta=4.0, l=8] Epoch 20/50, train_loss=0.3322, val_loss=0.8641 (Best: 0.6824)\n","[Propensity beta=4.0, l=8] Epoch 30/50, train_loss=0.1970, val_loss=1.1267 (Best: 0.6824)\n","[Propensity beta=4.0, l=8] Epoch 40/50, train_loss=0.1083, val_loss=1.4302 (Best: 0.6824)\n","[Propensity beta=4.0, l=8] Epoch 50/50, train_loss=0.0559, val_loss=1.7049 (Best: 0.6824)\n","[Propensity] Restored best model with val_loss=0.6824\n","[Propensity] Saved propensity model to /content/drive/MyDrive/ECE 685/Project/experiments_synthetic/propensity_models/propensity_beta4.0_l8.pt\n","[propensity run=0] Ep 50 | Loss: 210.5851 | MSE: 215.8141 | PEHE: 33.7804 (Best: 32.0965)\n","[propensity run=0] Ep 98 | Loss: 209.7436 | MSE: 214.6582 | PEHE: 35.0900 (Best: 32.0965)\n","Early stopping at epoch 98\n","[SAVE] Run 0 complete. Test PEHE: 35.0906\n"]}],"source":["import os\n","import json\n","from typing import Tuple, Optional, Dict, List\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# ============================================================\n","# Paths and basic configuration\n","# ============================================================\n","\n","ROOT = \"/content/drive/MyDrive/ECE 685/Project\"\n","\n","USE_SEMI_SYNTH = False\n","\n","if USE_SEMI_SYNTH:\n","    DATA_DIR   = os.path.join(ROOT, \"experiments_semi_synthetic\", \"data\")\n","    MODEL_DIR  = os.path.join(ROOT, \"experiments_semi_synthetic\", \"models\")\n","    RESULT_DIR = os.path.join(ROOT, \"experiments_semi_synthetic\", \"results\")\n","    PROP_DIR   = os.path.join(ROOT, \"experiments_semi_synthetic\", \"propensity_models\")\n","    FNAME_FMT  = \"semi_beta{beta}_l{l}.pt\"\n","else:\n","    DATA_DIR   = os.path.join(ROOT, \"experiments_synthetic\", \"data\")\n","    MODEL_DIR  = os.path.join(ROOT, \"experiments_synthetic\", \"models\")\n","    RESULT_DIR = os.path.join(ROOT, \"experiments_synthetic\", \"results\")\n","    PROP_DIR   = os.path.join(ROOT, \"experiments_synthetic\", \"propensity_models\")\n","    FNAME_FMT  = \"synthetic_beta{beta}_l{l}.pt\"\n","\n","os.makedirs(DATA_DIR, exist_ok=True)\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","os.makedirs(RESULT_DIR, exist_ok=True)\n","os.makedirs(PROP_DIR, exist_ok=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Training hyperparameters\n","BATCH_SIZE = 256\n","LR_TARNET = 5e-4\n","LR_PROP = 1e-3\n","N_EPOCHS_TARNET = 1000  # Set high, we rely on Early Stopping now\n","N_EPOCHS_PROP = 50\n","PATIENCE = 50           # Early stopping patience\n","LAMBDA_BAL = 100.0       # Increased from 1.0 to force balancing with strong bias\n","\n","\n","# ============================================================\n","# Utilities: loading synthetic datasets\n","# ============================================================\n","\n","def dataset_path(beta_norm: float, l: int) -> str:\n","    return os.path.join(DATA_DIR, FNAME_FMT.format(beta=beta_norm, l=l))\n","\n","\n","def load_synthetic_dataset(beta_norm: float, l: int):\n","    path = dataset_path(beta_norm, l)\n","    if not os.path.isfile(path):\n","        raise FileNotFoundError(f\"Dataset not found: {path}\")\n","    data = torch.load(path, map_location=\"cpu\")\n","\n","    X = data[\"X\"].float()  # (N, 1, H, W)\n","    t = data[\"T\"].float().view(-1, 1)  # (N, 1)\n","    y = data[\"Y\"].float().view(-1, 1)  # (N, 1)\n","\n","    if \"tau\" in data:\n","        tau = data[\"tau\"].float().view(-1, 1)\n","    else:\n","        mu0 = data[\"mu0\"].float().view(-1, 1)\n","        mu1 = data[\"mu1\"].float().view(-1, 1)\n","        tau = mu1 - mu0\n","\n","    return X, t, y, tau\n","\n","\n","def train_val_test_split(\n","    X: torch.Tensor,\n","    t: torch.Tensor,\n","    y: torch.Tensor,\n","    tau: torch.Tensor,\n","    train_frac: float = 0.6,\n","    val_frac: float = 0.2,\n","    seed: int = 1234,\n","):\n","    \"\"\"\n","    Simple random split into train/val/test with reproducible seed.\n","    \"\"\"\n","    N = X.shape[0]\n","    assert N == t.shape[0] == y.shape[0] == tau.shape[0]\n","\n","    g = torch.Generator().manual_seed(seed)\n","    perm = torch.randperm(N, generator=g)\n","\n","    n_train = int(train_frac * N)\n","    n_val = int(val_frac * N)\n","    # n_test = N - n_train - n_val\n","\n","    idx_train = perm[:n_train]\n","    idx_val = perm[n_train:n_train + n_val]\n","    idx_test = perm[n_train + n_val:]\n","\n","    data_train = (X[idx_train], t[idx_train], y[idx_train], tau[idx_train])\n","    data_val   = (X[idx_val],   t[idx_val],   y[idx_val],   tau[idx_val])\n","    data_test  = (X[idx_test],  t[idx_test],  y[idx_test],  tau[idx_test])\n","\n","    return data_train, data_val, data_test\n","\n","\n","# ============================================================\n","# Dataset object\n","# ============================================================\n","\n","class CausalDataset(Dataset):\n","    def __init__(\n","        self,\n","        X: torch.Tensor,\n","        t: torch.Tensor,\n","        y: torch.Tensor,\n","        tau: torch.Tensor,\n","        e_hat: Optional[torch.Tensor] = None,\n","    ):\n","        self.X = X\n","        self.t = t\n","        self.y = y\n","        self.tau = tau\n","        self.e_hat = e_hat\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = self.X[idx]\n","        t = self.t[idx]\n","        y = self.y[idx]\n","        tau = self.tau[idx]\n","        if self.e_hat is None:\n","            e = torch.tensor([0.0], dtype=torch.float32)  # dummy\n","        else:\n","            e = self.e_hat[idx]\n","        return x, t, y, tau, e\n","\n","\n","# ============================================================\n","# Models: representation, TARNet, propensity model\n","# ============================================================\n","\n","class ConvRepresentation(nn.Module):\n","    def __init__(self, rep_dim: int = 64):\n","        super().__init__()\n","        # === CHANGE 1: SIMPLER CNN ===\n","        # Reduced filters from [16, 32, 64] to [8, 16, 32] to reduce capacity\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(32, rep_dim), # Input matches last conv channel count\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x):\n","        h = self.features(x)\n","        h = self.fc(h)\n","        return h  # (N, rep_dim)\n","\n","\n","class TARNet(nn.Module):\n","    def __init__(self, rep_dim: int = 64, use_propensity: bool = False):\n","        super().__init__()\n","        self.use_propensity = use_propensity\n","        self.rep = ConvRepresentation(rep_dim=rep_dim)\n","\n","        in_dim_heads = rep_dim + 1 if use_propensity else rep_dim\n","\n","        # === CHANGE 2: ADD DROPOUT ===\n","        # Added Dropout(0.3) to prevent overfitting in the heads\n","        self.head0 = nn.Sequential(\n","            nn.Linear(in_dim_heads, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1),\n","        )\n","        self.head1 = nn.Sequential(\n","            nn.Linear(in_dim_heads, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1),\n","        )\n","\n","    def forward(self, x, e_hat=None):\n","        phi_x = self.rep(x)\n","        if self.use_propensity:\n","            if e_hat is None:\n","                raise ValueError(\"e_hat must be provided when use_propensity=True\")\n","            inp = torch.cat([phi_x, e_hat], dim=1)\n","        else:\n","            inp = phi_x\n","        y0 = self.head0(inp)\n","        y1 = self.head1(inp)\n","        return y0, y1, phi_x\n","\n","\n","class PropensityMLP(nn.Module):\n","    \"\"\"\n","    Simple propensity model using flattened images -> 2-layer MLP.\n","    \"\"\"\n","    def __init__(self, input_dim: int):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1),\n","        )\n","\n","    def forward(self, x):\n","        flat = x.view(x.size(0), -1)\n","        logits = self.net(flat)\n","        return logits\n","\n","\n","# ============================================================\n","# Losses and metrics\n","# ============================================================\n","def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n","    \"\"\"Compute pairwise euclidean distance matrix\"\"\"\n","    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n","    norm = float(norm)\n","    expansion_1 = sample_1.unsqueeze(1).expand(n_1, n_2, -1)\n","    expansion_2 = sample_2.unsqueeze(0).expand(n_1, n_2, -1)\n","    return torch.norm(expansion_1 - expansion_2, p=norm, dim=2)\n","\n","def balancing_loss(phi_x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    RBF MMD Loss with Median Heuristic.\n","    \"\"\"\n","    t = t.view(-1)\n","    treated = phi_x[t > 0.5]\n","    control = phi_x[t <= 0.5]\n","\n","    if treated.size(0) == 0 or control.size(0) == 0:\n","        return torch.tensor(0.0, device=phi_x.device)\n","\n","    # 1. Dynamic Sigma Calculation\n","    all_samples = torch.cat([treated, control], dim=0)\n","    dists = pdist(all_samples, all_samples)\n","\n","    n = dists.shape[0]\n","    mask = ~torch.eye(n, dtype=torch.bool, device=phi_x.device)\n","    median_dist = dists[mask].median()\n","\n","    sigma = median_dist.detach()\n","    if sigma == 0: sigma = torch.tensor(1.0, device=phi_x.device)\n","\n","    # 2. Compute MMD\n","    D_tt = pdist(treated, treated)\n","    D_tc = pdist(treated, control)\n","    D_cc = pdist(control, control)\n","\n","    K_tt = torch.exp(-D_tt**2 / (2 * sigma**2)).mean()\n","    K_tc = torch.exp(-D_tc**2 / (2 * sigma**2)).mean()\n","    K_cc = torch.exp(-D_cc**2 / (2 * sigma**2)).mean()\n","\n","    return K_tt - 2 * K_tc + K_cc\n","\n","\n","\n","\n","\n","def compute_pehe(model: TARNet, dataloader: DataLoader, use_propensity: bool) -> float:\n","    \"\"\"\n","    Compute PEHE on a given dataloader.\n","    \"\"\"\n","    model.eval()\n","    sq_errors = []\n","\n","    with torch.no_grad():\n","        for x, t, y, tau, e_hat in dataloader:\n","            x = x.to(device)\n","            tau = tau.to(device)\n","            e_hat = e_hat.to(device)\n","\n","            if use_propensity:\n","                y0_hat, y1_hat, _ = model(x, e_hat=e_hat)\n","            else:\n","                y0_hat, y1_hat, _ = model(x, e_hat=None)\n","\n","            tau_hat = (y1_hat - y0_hat)\n","            sq_errors.append((tau_hat - tau) ** 2)\n","\n","    if len(sq_errors) == 0:\n","        return float(\"nan\")\n","\n","    sq_errors = torch.cat(sq_errors, dim=0)\n","    pehe = torch.sqrt(sq_errors.mean()).item()\n","    return pehe\n","\n","\n","def compute_mse_factual(model: TARNet, dataloader: DataLoader, use_propensity: bool) -> float:\n","    model.eval()\n","    mse = nn.MSELoss(reduction=\"sum\")\n","    total_loss = 0.0\n","    total_n = 0\n","\n","    with torch.no_grad():\n","        for x, t, y, tau, e_hat in dataloader:\n","            x = x.to(device)\n","            t = t.to(device)\n","            y = y.to(device)\n","            e_hat = e_hat.to(device)\n","\n","            if use_propensity:\n","                y0_hat, y1_hat, _ = model(x, e_hat=e_hat)\n","            else:\n","                y0_hat, y1_hat, _ = model(x, e_hat=None)\n","\n","            y_hat = torch.where(t > 0.5, y1_hat, y0_hat)\n","            loss = mse(y_hat, y)\n","            total_loss += loss.item()\n","            total_n += y.shape[0]\n","\n","    if total_n == 0:\n","        return float(\"nan\")\n","    return total_loss / total_n\n","\n","\n","# ============================================================\n","# Training helpers\n","# ============================================================\n","\n","def get_or_train_propensity_model(\n","    beta_norm: float,\n","    l: int,\n","    train_dataset: CausalDataset,\n","    val_dataset: CausalDataset,\n","    img_size: Tuple[int, int],\n",") -> PropensityMLP:\n","    \"\"\"\n","    Train or load a propensity model.\n","    UPDATED: Uses Weight Decay, Lower LR, and Best Model Checkpointing.\n","    \"\"\"\n","    H, W = img_size\n","    input_dim = 1 * H * W\n","    prop_path = os.path.join(PROP_DIR, f\"propensity_beta{beta_norm}_l{l}.pt\")\n","\n","    prop_model = PropensityMLP(input_dim=input_dim).to(device)\n","\n","    # if os.path.isfile(prop_path):\n","    #     print(f\"[Propensity] Loading existing propensity model from {prop_path}\")\n","    #     state = torch.load(prop_path, map_location=device)\n","    #     prop_model.load_state_dict(state)\n","    #     return prop_model\n","\n","    print(f\"[Propensity] Training new propensity model for beta={beta_norm}, l={l}\")\n","\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # === CHANGE 1 & 2: Lower LR and Add Weight Decay ===\n","    # Reduced LR to 1e-4 for stability\n","    # Added weight_decay=1e-4 for regularization\n","    optimizer = torch.optim.Adam(prop_model.parameters(), lr=1e-4, weight_decay=1e-4)\n","    bce = nn.BCEWithLogitsLoss()\n","\n","    # === CHANGE 3: Track Best Model ===\n","    best_val_loss = float('inf')\n","    best_state = None\n","\n","    for epoch in range(N_EPOCHS_PROP):\n","        prop_model.train()\n","        total_loss = 0.0\n","        n = 0\n","\n","        for x, t, y, tau, e_hat in train_loader:\n","            x = x.to(device)\n","            t = t.to(device)\n","\n","            logits = prop_model(x)\n","            loss = bce(logits, t)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * x.size(0)\n","            n += x.size(0)\n","\n","        avg_train_loss = total_loss / n if n > 0 else 0.0\n","\n","        # Validation Loop\n","        prop_model.eval()\n","        total_val_loss = 0.0\n","        n_val = 0\n","        with torch.no_grad():\n","            for x, t, y, tau, e_hat in val_loader:\n","                x = x.to(device)\n","                t = t.to(device)\n","                logits = prop_model(x)\n","                loss = bce(logits, t)\n","                total_val_loss += loss.item() * x.size(0)\n","                n_val += x.size(0)\n","\n","        avg_val_loss = total_val_loss / n_val if n_val > 0 else 0.0\n","\n","        # Save Best State\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            best_state = prop_model.state_dict()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(\n","                f\"[Propensity beta={beta_norm}, l={l}] \"\n","                f\"Epoch {epoch+1}/{N_EPOCHS_PROP}, \"\n","                f\"train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f} \"\n","                f\"(Best: {best_val_loss:.4f})\"\n","            )\n","\n","    # === CRITICAL: Restore Best State ===\n","    if best_state is not None:\n","        prop_model.load_state_dict(best_state)\n","        print(f\"[Propensity] Restored best model with val_loss={best_val_loss:.4f}\")\n","\n","    torch.save(prop_model.state_dict(), prop_path)\n","    print(f\"[Propensity] Saved propensity model to {prop_path}\")\n","    return prop_model\n","\n","def train_tarnet_single_model(\n","    beta_norm: float,\n","    l: int,\n","    model_type: str,\n","    run_id: int = 0,         # Added for Multi-Seed Support\n","    lr: float = 1e-3,        # Added to allow custom LR per model if needed\n","    seed_split: int = 2025,\n",") -> None:\n","    \"\"\"\n","    Train a single TARNet model with robust checkpointing and regularization.\n","    \"\"\"\n","    assert model_type in [\"baseline\", \"baseline_bal\", \"propensity\", \"propensity_bal\"]\n","\n","    # Unique filename for this specific run (includes run_id)\n","    model_filename = f\"tarnet_{model_type}_beta{beta_norm}_l{l}.pt\"\n","    model_path = os.path.join(MODEL_DIR, model_filename)\n","\n","    metrics_filename = f\"metrics_{model_type}_beta{beta_norm}_l{l}.json\"\n","    metrics_path = os.path.join(RESULT_DIR, metrics_filename)\n","\n","    # if os.path.isfile(model_path) and os.path.isfile(metrics_path):\n","    #     print(f\"[SKIP] Run {run_id} already exists for {model_type}, beta={beta_norm}, l={l}\")\n","    #     return\n","\n","    print(f\"\\n[TRAIN] beta={beta_norm}, l={l}, type={model_type}, run={run_id}\")\n","\n","    # 1. Load Data\n","    X, t, y, tau = load_synthetic_dataset(beta_norm, l)\n","    N, C, H, W = X.shape\n","\n","    # 2. Deterministic Split (Unique per run_id)\n","    # Using run_id in the seed ensures different splits for different runs\n","    current_seed = seed_split + int(beta_norm * 100) + l + (run_id * 1000)\n","\n","    (X_tr, t_tr, y_tr, tau_tr), \\\n","    (X_val, t_val, y_val, tau_val), \\\n","    (X_te, t_te, y_te, tau_te) = train_val_test_split(\n","        X, t, y, tau, seed=current_seed\n","    )\n","\n","    # 3. Handle Propensity\n","    use_propensity = model_type.startswith(\"propensity\")\n","    if use_propensity:\n","        tmp_train_ds = CausalDataset(X_tr, t_tr, y_tr, tau_tr, e_hat=None)\n","        tmp_val_ds = CausalDataset(X_val, t_val, y_val, tau_val, e_hat=None)\n","\n","        prop_model = get_or_train_propensity_model(\n","            beta_norm=beta_norm,\n","            l=l,\n","            train_dataset=tmp_train_ds,\n","            val_dataset=tmp_val_ds,\n","            img_size=(H, W),\n","        ).to(device)\n","\n","        prop_model.eval()\n","        with torch.no_grad():\n","            e_tr = torch.sigmoid(prop_model(X_tr.to(device))).cpu()\n","            e_val = torch.sigmoid(prop_model(X_val.to(device))).cpu()\n","            e_te = torch.sigmoid(prop_model(X_te.to(device))).cpu()\n","\n","            # --- FIX: Clip Propensity Scores ---\n","            # Prevents instability when score is exactly 0.0 or 1.0\n","            e_tr = torch.clamp(e_tr, 0.05, 0.95)\n","            e_val = torch.clamp(e_val, 0.05, 0.95)\n","            e_te = torch.clamp(e_te, 0.05, 0.95)\n","    else:\n","        e_tr = e_val = e_te = None\n","\n","    # 4. Data Loaders\n","    train_ds = CausalDataset(X_tr, t_tr, y_tr, tau_tr, e_hat=e_tr)\n","    val_ds   = CausalDataset(X_val, t_val, y_val, tau_val, e_hat=e_val)\n","    test_ds  = CausalDataset(X_te, t_te, y_te, tau_te, e_hat=e_te)\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # 5. Model Setup\n","    model = TARNet(rep_dim=64, use_propensity=use_propensity).to(device)\n","\n","    # --- FIX: Add Weight Decay (L2 Regularization) ---\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n","    mse = nn.MSELoss()\n","\n","    use_balancing = model_type.endswith(\"_bal\")\n","\n","    # 6. Training Loop with Burn-in & Early Stopping\n","    best_val_pehe = float(\"inf\")\n","    best_state = None\n","    triggers = 0\n","\n","    # Burn-in: Ignore validation metrics for first 20 epochs to avoid \"lucky\" initialization checkpoints\n","    BURN_IN_EPOCHS = 1\n","\n","    for epoch in range(N_EPOCHS_TARNET):\n","        model.train()\n","        total_train_loss = 0.0\n","        n_train = 0\n","\n","        for x, t_batch, y_batch, tau_batch, e_hat_batch in train_loader:\n","            x = x.to(device)\n","            t_batch = t_batch.to(device)\n","            y_batch = y_batch.to(device)\n","            e_hat_batch = e_hat_batch.to(device)\n","\n","            if use_propensity:\n","                y0_hat, y1_hat, phi_x = model(x, e_hat=e_hat_batch)\n","            else:\n","                y0_hat, y1_hat, phi_x = model(x, e_hat=None)\n","\n","            y_pred = torch.where(t_batch > 0.5, y1_hat, y0_hat)\n","            factual_loss = mse(y_pred, y_batch)\n","\n","            if use_balancing:\n","                bal_loss = balancing_loss(phi_x, t_batch)\n","                loss = factual_loss + LAMBDA_BAL * bal_loss\n","            else:\n","                loss = factual_loss\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_train_loss += loss.item() * x.size(0)\n","            n_train += x.size(0)\n","\n","        avg_train_loss = total_train_loss / n_train if n_train > 0 else 0.0\n","\n","        # --- Validation & Checkpointing ---\n","        val_pehe = compute_pehe(model, val_loader, use_propensity=use_propensity)\n","        val_mse_f = compute_mse_factual(model, val_loader, use_propensity=use_propensity)\n","\n","        # Only check for best model AFTER burn-in period\n","        if epoch >= BURN_IN_EPOCHS:\n","            if val_pehe < best_val_pehe:\n","                best_val_pehe = val_pehe\n","                best_state = model.state_dict()\n","                triggers = 0\n","            else:\n","                triggers += 1\n","        else:\n","            # During burn-in, reset triggers (don't early stop)\n","            triggers = 0\n","\n","        # Logging\n","        if (epoch + 1) % 50 == 0 or triggers >= PATIENCE:\n","            status = \"(Burn-in)\" if epoch < BURN_IN_EPOCHS else f\"(Best: {best_val_pehe:.4f})\"\n","            print(\n","                f\"[{model_type} run={run_id}] Ep {epoch+1} | \"\n","                f\"Loss: {avg_train_loss:.4f} | MSE: {val_mse_f:.4f} | \"\n","                f\"PEHE: {val_pehe:.4f} {status}\"\n","            )\n","\n","        if triggers >= PATIENCE:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # 7. Final Evaluation (Load Best State)\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    else:\n","        print(\"[WARNING] No best state found (did not pass burn-in?). Using last state.\")\n","\n","    train_pehe = compute_pehe(model, train_loader, use_propensity=use_propensity)\n","    val_pehe_final = compute_pehe(model, val_loader, use_propensity=use_propensity)\n","    test_pehe = compute_pehe(model, test_loader, use_propensity=use_propensity)\n","\n","    metrics = {\n","        \"beta_norm\": beta_norm,\n","        \"l\": l,\n","        \"model_type\": model_type,\n","        \"run_id\": run_id,\n","        \"train_PEHE\": train_pehe,\n","        \"val_PEHE\": val_pehe_final,\n","        \"best_val_PEHE\": best_val_pehe, # Explicitly save the tracked minimum\n","        \"test_PEHE\": test_pehe,\n","        \"epochs\": N_EPOCHS_TARNET,\n","        \"lambda_bal\": LAMBDA_BAL,\n","        \"lr\": lr\n","    }\n","\n","    torch.save(model.state_dict(), model_path)\n","    with open(metrics_path, \"w\") as f:\n","        json.dump(metrics, f, indent=2)\n","\n","    print(f\"[SAVE] Run {run_id} complete. Test PEHE: {test_pehe:.4f}\")\n","\n","\n","# ============================================================\n","# Main loops\n","# ============================================================\n","\n","if __name__ == \"__main__\":\n","    beta_list = [0.5, 1.0, 2.0, 4.0]\n","    l_list = [0, 1, 2, 4, 8]\n","    model_types = [\"baseline\", \"baseline_bal\", \"propensity\"]  #\"propensity_bal\"]\n","\n","    '''\n","    for l in l_list\n","        train_tarnet_single_model(beta_norm=1.0, l=l, model_type=\"baseline\")\n","        train_tarnet_single_model(beta_norm=1.0, l=l, model_type=\"baseline_bal\")\n","        train_tarnet_single_model(beta_norm=1.0, l=l, model_type=\"propensity\")\n","    '''\n","\n","    '''\n","    for beta_norm in beta_list:\n","        for l in l_list:\n","            train_tarnet_single_model(beta_norm=beta_norm, l=l, model_type=\"baseline_bal\")\n","    '''\n","\n","\n","    for beta_norm in beta_list:\n","        for l in l_list:\n","            for mtype in model_types:\n","                train_tarnet_single_model(beta_norm=beta_norm, l=l, model_type=mtype)\n",""]},{"cell_type":"code","source":[],"metadata":{"id":"2SgPO7j06cWv"},"execution_count":null,"outputs":[]}]}