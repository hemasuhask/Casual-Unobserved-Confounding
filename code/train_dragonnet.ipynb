{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNblO1+tD2Cit3Fy0vnzpGz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQtb1P4BkPhE","executionInfo":{"status":"ok","timestamp":1764994512021,"user_tz":360,"elapsed":13319,"user":{"displayName":"Lin Z","userId":"11285634647307012657"}},"outputId":"c3601d29-508b-4a85-8862-8055926bbba2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","from typing import Tuple, Optional, Dict, List\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","# ============================================================\n","# Paths and basic configuration\n","# ============================================================\n","\n","ROOT = \"/content/drive/MyDrive/ECE 685/Project\"\n","\n","USE_SEMI_SYNTH = False\n","\n","if USE_SEMI_SYNTH:\n","    DATA_DIR   = os.path.join(ROOT, \"experiments_semi_synthetic\", \"data\")\n","    MODEL_DIR  = os.path.join(ROOT, \"experiments_semi_synthetic\", \"models\")\n","    RESULT_DIR = os.path.join(ROOT, \"experiments_semi_synthetic\", \"results\")\n","    FNAME_FMT  = \"semi_beta{beta}_l{l}.pt\"\n","else:\n","    DATA_DIR   = os.path.join(ROOT, \"experiments_synthetic\", \"data\")\n","    MODEL_DIR  = os.path.join(ROOT, \"experiments_synthetic\", \"models\")\n","    RESULT_DIR = os.path.join(ROOT, \"experiments_synthetic\", \"results\")\n","    FNAME_FMT  = \"synthetic_beta{beta}_l{l}.pt\"\n","\n","os.makedirs(DATA_DIR, exist_ok=True)\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","os.makedirs(RESULT_DIR, exist_ok=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Training hyperparameters\n","BATCH_SIZE = 256\n","LR_DRAGON = 1e-3\n","N_EPOCHS = 1000\n","PATIENCE = 50\n","WEIGHT_DECAY = 1e-4\n","TARGET_REG_ALPHA = 1.0  # Coefficient for targeted regularization loss (t-reg)\n","\n","# ============================================================\n","# Utilities: loading synthetic datasets\n","# ============================================================\n","\n","def dataset_path(beta_norm: float, l: int) -> str:\n","    return os.path.join(DATA_DIR, FNAME_FMT.format(beta=beta_norm, l=l))\n","\n","def load_synthetic_dataset(beta_norm: float, l: int):\n","    path = dataset_path(beta_norm, l)\n","    if not os.path.isfile(path):\n","        raise FileNotFoundError(f\"Dataset not found: {path}\")\n","    data = torch.load(path, map_location=\"cpu\")\n","\n","    X = data[\"X\"].float()  # (N, 1, H, W)\n","    t = data[\"T\"].float().view(-1, 1)  # (N, 1)\n","    y = data[\"Y\"].float().view(-1, 1)  # (N, 1)\n","\n","    if \"tau\" in data:\n","        tau = data[\"tau\"].float().view(-1, 1)\n","    else:\n","        mu0 = data[\"mu0\"].float().view(-1, 1)\n","        mu1 = data[\"mu1\"].float().view(-1, 1)\n","        tau = mu1 - mu0\n","\n","    return X, t, y, tau\n","\n","def train_val_test_split(\n","    X: torch.Tensor,\n","    t: torch.Tensor,\n","    y: torch.Tensor,\n","    tau: torch.Tensor,\n","    train_frac: float = 0.6,\n","    val_frac: float = 0.2,\n","    seed: int = 1234,\n","):\n","    N = X.shape[0]\n","    g = torch.Generator().manual_seed(seed)\n","    perm = torch.randperm(N, generator=g)\n","\n","    n_train = int(train_frac * N)\n","    n_val = int(val_frac * N)\n","\n","    idx_train = perm[:n_train]\n","    idx_val = perm[n_train:n_train + n_val]\n","    idx_test = perm[n_train + n_val:]\n","\n","    data_train = (X[idx_train], t[idx_train], y[idx_train], tau[idx_train])\n","    data_val   = (X[idx_val],   t[idx_val],   y[idx_val],   tau[idx_val])\n","    data_test  = (X[idx_test],  t[idx_test],  y[idx_test],  tau[idx_test])\n","\n","    return data_train, data_val, data_test\n","\n","# ============================================================\n","# Dataset object\n","# ============================================================\n","\n","class CausalDataset(Dataset):\n","    def __init__(self, X, t, y, tau):\n","        self.X = X\n","        self.t = t\n","        self.y = y\n","        self.tau = tau\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.t[idx], self.y[idx], self.tau[idx]\n","\n","# ============================================================\n","# Dragonnet Model\n","# ============================================================\n","\n","class ConvRepresentation(nn.Module):\n","    def __init__(self, rep_dim: int = 64):\n","        super().__init__()\n","        # Matches your TARNet encoder capacity\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(32, rep_dim),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x):\n","        h = self.features(x)\n","        h = self.fc(h)\n","        return h\n","\n","class Dragonnet(nn.Module):\n","    \"\"\"\n","    Dragonnet: A 3-headed architecture.\n","    - Shared Representation Z\n","    - Propensity Head g(Z) -> predicts T\n","    - Outcome Head Q0(Z) -> predicts Y | T=0\n","    - Outcome Head Q1(Z) -> predicts Y | T=1\n","    \"\"\"\n","    def __init__(self, rep_dim: int = 64):\n","        super().__init__()\n","        self.rep = ConvRepresentation(rep_dim=rep_dim)\n","\n","        # Propensity Head (g)\n","        self.propensity_head = nn.Sequential(\n","            nn.Linear(rep_dim, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1)\n","        )\n","\n","        # Outcome Head 0 (Q0)\n","        self.head0 = nn.Sequential(\n","            nn.Linear(rep_dim, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1)\n","        )\n","\n","        # Outcome Head 1 (Q1)\n","        self.head1 = nn.Sequential(\n","            nn.Linear(rep_dim, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        z = self.rep(x)\n","\n","        # Propensity logits\n","        g_logits = self.propensity_head(z)\n","\n","        # Outcome predictions (Hypothesis)\n","        y0 = self.head0(z)\n","        y1 = self.head1(z)\n","\n","        return y0, y1, g_logits\n","\n","# ============================================================\n","# Losses and Metrics\n","# ============================================================\n","\n","def dragonnet_loss(y0, y1, g_logits, t, y):\n","    \"\"\"\n","    Loss = L_outcome + L_propensity\n","    L_outcome = sum((y - y_pred)^2)\n","    L_propensity = Binary Cross Entropy\n","    \"\"\"\n","    # 1. Outcome Loss (Standard MSE on factuals)\n","    y_pred = torch.where(t > 0.5, y1, y0)\n","    loss_y = F.mse_loss(y_pred, y)\n","\n","    # 2. Propensity Loss\n","    loss_g = F.binary_cross_entropy_with_logits(g_logits, t)\n","\n","    return loss_y, loss_g\n","\n","def targeted_regularization(y0, y1, g_logits, t, y):\n","    \"\"\"\n","    Targeted Regularization (t-reg) specifically for Dragonnet.\n","    Based on Shi et al. (2019).\n","    \"\"\"\n","    g = torch.sigmoid(g_logits)\n","    # Epsilon calculation (perturbation parameter)\n","    # epsilon = (y - Q(t,x)) / (t - g(x)) ??? No, standard implementation uses specific closed form or update\n","\n","    # Standard implementation often does this step separately or adds a specific term.\n","    # Here we implement the basic loss structure from the paper:\n","    # L = L_y + alpha * L_g + beta * L_treg\n","\n","    # For this simplified implementation, we stick to the primary Dragonnet loss:\n","    # L = MSE(y) + BCE(t)\n","    # The 'targeted' part is often an update step, but basic Dragonnet works with just the multi-task loss.\n","    # We will return 0 here unless full TMLE logic is needed.\n","    return torch.tensor(0.0).to(y.device)\n","\n","def compute_pehe(model: Dragonnet, dataloader: DataLoader) -> float:\n","    model.eval()\n","    sq_errors = []\n","    with torch.no_grad():\n","        for x, t, y, tau in dataloader:\n","            x = x.to(device)\n","            tau = tau.to(device)\n","            y0, y1, _ = model(x)\n","            tau_hat = y1 - y0\n","            sq_errors.append((tau_hat - tau) ** 2)\n","\n","    if len(sq_errors) == 0: return float(\"nan\")\n","    sq_errors = torch.cat(sq_errors, dim=0)\n","    return torch.sqrt(sq_errors.mean()).item()\n","\n","# ============================================================\n","# Training Function\n","# ============================================================\n","\n","def train_dragonnet_single_model(\n","    beta_norm: float,\n","    l: int,\n","    run_id: int = 0,\n","    lr: float = 1e-3,\n","    seed_split: int = 2025,\n",") -> None:\n","\n","    model_filename = f\"dragonnet_beta{beta_norm}_l{l}.pt\"\n","    model_path = os.path.join(MODEL_DIR, model_filename)\n","    metrics_filename = f\"metrics_dragonnet_beta{beta_norm}_l{l}.json\"\n","    metrics_path = os.path.join(RESULT_DIR, metrics_filename)\n","\n","    if os.path.isfile(model_path) and os.path.isfile(metrics_path):\n","        print(f\"[SKIP] Dragonnet run {run_id} already exists for beta={beta_norm}, l={l}\")\n","        return\n","\n","    print(f\"\\n[TRAIN DRAGONNET] beta={beta_norm}, l={l}, run={run_id}\")\n","\n","    # 1. Load Data\n","    X, t, y, tau = load_synthetic_dataset(beta_norm, l)\n","\n","    # 2. Split\n","    current_seed = seed_split + int(beta_norm * 100) + l + (run_id * 1000)\n","    (X_tr, t_tr, y_tr, tau_tr), (X_val, t_val, y_val, tau_val), (X_te, t_te, y_te, tau_te) = \\\n","        train_val_test_split(X, t, y, tau, seed=current_seed)\n","\n","    train_loader = DataLoader(CausalDataset(X_tr, t_tr, y_tr, tau_tr), batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(CausalDataset(X_val, t_val, y_val, tau_val), batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader = DataLoader(CausalDataset(X_te, t_te, y_te, tau_te), batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # 3. Model & Optimizer\n","    model = Dragonnet(rep_dim=64).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n","\n","    # 4. Training Loop\n","    best_val_pehe = float(\"inf\")\n","    best_state = None\n","    triggers = 0\n","    BURN_IN = 1\n","\n","    for epoch in range(N_EPOCHS):\n","        model.train()\n","        total_loss = 0.0\n","        total_loss_y = 0.0\n","        total_loss_g = 0.0\n","        n_train = 0\n","\n","        for x, t, y, tau in train_loader:\n","            x, t, y = x.to(device), t.to(device), y.to(device)\n","\n","            y0, y1, g_logits = model(x)\n","\n","            loss_y, loss_g = dragonnet_loss(y0, y1, g_logits, t, y)\n","\n","            # Dragonnet Objective: Minimize both outcome error and propensity error\n","            # This forces the representation Z to be predictive of BOTH Y and T.\n","            # (Standard alpha=1.0)\n","            loss = loss_y + 1.0 * loss_g\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * x.size(0)\n","            total_loss_y += loss_y.item() * x.size(0)\n","            total_loss_g += loss_g.item() * x.size(0)\n","            n_train += x.size(0)\n","\n","        # Validation\n","        val_pehe = compute_pehe(model, val_loader)\n","\n","        # Checkpointing\n","        if epoch >= BURN_IN:\n","            if val_pehe < best_val_pehe:\n","                best_val_pehe = val_pehe\n","                best_state = model.state_dict()\n","                triggers = 0\n","            else:\n","                triggers += 1\n","        else:\n","            triggers = 0\n","\n","        if (epoch + 1) % 50 == 0 or triggers >= PATIENCE:\n","            status = \"(Burn-in)\" if epoch < BURN_IN else f\"(Best: {best_val_pehe:.4f})\"\n","            print(f\"[Dragonnet] Ep {epoch+1} | Loss: {total_loss/n_train:.4f} (Y:{total_loss_y/n_train:.2f}, G:{total_loss_g/n_train:.2f}) | PEHE: {val_pehe:.4f} {status}\")\n","\n","        if triggers >= PATIENCE:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # 5. Save Results\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    test_pehe = compute_pehe(model, test_loader)\n","\n","    metrics = {\n","        \"beta_norm\": beta_norm,\n","        \"l\": l,\n","        \"model_type\": \"dragonnet\",\n","        \"run_id\": run_id,\n","        \"val_PEHE\": best_val_pehe, # Save best valid\n","        \"test_PEHE\": test_pehe,\n","        \"epochs\": N_EPOCHS,\n","    }\n","\n","    torch.save(model.state_dict(), model_path)\n","    with open(metrics_path, \"w\") as f:\n","        json.dump(metrics, f, indent=2)\n","\n","    print(f\"[SAVE] Run {run_id} complete. Test PEHE: {test_pehe:.4f}\")\n","\n","# ============================================================\n","# Main Execution\n","# ============================================================\n","\n","if __name__ == \"__main__\":\n","    beta_list = [0.5, 1.0, 2.0, 4.0]\n","    l_list = [0, 1, 2, 4, 8]\n","    N_RUNS = 1\n","\n","    for beta in beta_list:\n","        for l in l_list:\n","            train_dragonnet_single_model(beta, l, run_id=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQL2iavfkQqF","executionInfo":{"status":"ok","timestamp":1764995150476,"user_tz":360,"elapsed":631488,"user":{"displayName":"Lin Z","userId":"11285634647307012657"}},"outputId":"9f0e5c2c-7340-41f9-c482-bf6ece5c78c3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","[TRAIN DRAGONNET] beta=0.5, l=0, run=0\n","[Dragonnet] Ep 50 | Loss: 0.7072 (Y:0.02, G:0.69) | PEHE: 0.0091 (Best: 0.0047)\n","[Dragonnet] Ep 100 | Loss: 0.7049 (Y:0.01, G:0.69) | PEHE: 0.0380 (Best: 0.0039)\n","[Dragonnet] Ep 150 | Loss: 0.7032 (Y:0.01, G:0.69) | PEHE: 0.0038 (Best: 0.0038)\n","[Dragonnet] Ep 200 | Loss: 0.7017 (Y:0.01, G:0.69) | PEHE: 0.0038 (Best: 0.0038)\n","Early stopping at epoch 200\n","[SAVE] Run 0 complete. Test PEHE: 0.0037\n","\n","[TRAIN DRAGONNET] beta=0.5, l=1, run=0\n","[Dragonnet] Ep 50 | Loss: 1.6678 (Y:0.98, G:0.69) | PEHE: 0.4432 (Best: 0.1830)\n","[Dragonnet] Ep 52 | Loss: 1.6671 (Y:0.98, G:0.69) | PEHE: 0.3750 (Best: 0.1830)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 0.3749\n","\n","[TRAIN DRAGONNET] beta=0.5, l=2, run=0\n","[Dragonnet] Ep 50 | Loss: 2.4979 (Y:1.81, G:0.69) | PEHE: 0.7944 (Best: 0.5290)\n","[Dragonnet] Ep 81 | Loss: 2.5002 (Y:1.81, G:0.69) | PEHE: 0.7578 (Best: 0.5290)\n","Early stopping at epoch 81\n","[SAVE] Run 0 complete. Test PEHE: 0.7580\n","\n","[TRAIN DRAGONNET] beta=0.5, l=4, run=0\n","[Dragonnet] Ep 50 | Loss: 4.0970 (Y:3.41, G:0.69) | PEHE: 1.5783 (Best: 1.1619)\n","[Dragonnet] Ep 52 | Loss: 4.0968 (Y:3.41, G:0.69) | PEHE: 1.5722 (Best: 1.1619)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 1.5720\n","\n","[TRAIN DRAGONNET] beta=0.5, l=8, run=0\n","[Dragonnet] Ep 50 | Loss: 6.8248 (Y:6.13, G:0.69) | PEHE: 2.5916 (Best: 2.3849)\n","[Dragonnet] Ep 71 | Loss: 6.8293 (Y:6.14, G:0.69) | PEHE: 2.7000 (Best: 2.3849)\n","Early stopping at epoch 71\n","[SAVE] Run 0 complete. Test PEHE: 2.7018\n","\n","[TRAIN DRAGONNET] beta=1.0, l=0, run=0\n","[Dragonnet] Ep 50 | Loss: 0.7071 (Y:0.02, G:0.69) | PEHE: 0.0180 (Best: 0.0040)\n","[Dragonnet] Ep 100 | Loss: 0.7054 (Y:0.01, G:0.69) | PEHE: 0.0107 (Best: 0.0037)\n","[Dragonnet] Ep 150 | Loss: 0.7035 (Y:0.01, G:0.69) | PEHE: 0.0060 (Best: 0.0037)\n","[Dragonnet] Ep 200 | Loss: 0.7026 (Y:0.01, G:0.69) | PEHE: 0.0037 (Best: 0.0036)\n","[Dragonnet] Ep 250 | Loss: 0.7023 (Y:0.01, G:0.69) | PEHE: 0.0060 (Best: 0.0036)\n","[Dragonnet] Ep 253 | Loss: 0.7023 (Y:0.01, G:0.69) | PEHE: 0.0040 (Best: 0.0036)\n","Early stopping at epoch 253\n","[SAVE] Run 0 complete. Test PEHE: 0.0041\n","\n","[TRAIN DRAGONNET] beta=1.0, l=1, run=0\n","[Dragonnet] Ep 50 | Loss: 4.1411 (Y:3.45, G:0.69) | PEHE: 1.5931 (Best: 1.1978)\n","[Dragonnet] Ep 52 | Loss: 4.1509 (Y:3.46, G:0.69) | PEHE: 1.6220 (Best: 1.1978)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 1.6219\n","\n","[TRAIN DRAGONNET] beta=1.0, l=2, run=0\n","[Dragonnet] Ep 50 | Loss: 6.7688 (Y:6.08, G:0.69) | PEHE: 2.5471 (Best: 2.3890)\n","[Dragonnet] Ep 99 | Loss: 6.6723 (Y:5.98, G:0.69) | PEHE: 2.7411 (Best: 2.3890)\n","Early stopping at epoch 99\n","[SAVE] Run 0 complete. Test PEHE: 2.7221\n","\n","[TRAIN DRAGONNET] beta=1.0, l=4, run=0\n","[Dragonnet] Ep 50 | Loss: 11.2096 (Y:10.52, G:0.69) | PEHE: 4.4906 (Best: 3.6462)\n","[Dragonnet] Ep 52 | Loss: 11.1952 (Y:10.51, G:0.69) | PEHE: 4.9028 (Best: 3.6462)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 4.9039\n","\n","[TRAIN DRAGONNET] beta=1.0, l=8, run=0\n","[Dragonnet] Ep 50 | Loss: 18.5012 (Y:17.81, G:0.69) | PEHE: 7.5771 (Best: 6.2045)\n","[Dragonnet] Ep 73 | Loss: 18.6195 (Y:17.93, G:0.69) | PEHE: 7.4296 (Best: 6.2045)\n","Early stopping at epoch 73\n","[SAVE] Run 0 complete. Test PEHE: 7.4313\n","\n","[TRAIN DRAGONNET] beta=2.0, l=0, run=0\n","[Dragonnet] Ep 50 | Loss: 0.7060 (Y:0.01, G:0.69) | PEHE: 0.0069 (Best: 0.0043)\n","[Dragonnet] Ep 100 | Loss: 0.7042 (Y:0.01, G:0.69) | PEHE: 0.0220 (Best: 0.0038)\n","[Dragonnet] Ep 150 | Loss: 0.7026 (Y:0.01, G:0.69) | PEHE: 0.0048 (Best: 0.0038)\n","[Dragonnet] Ep 200 | Loss: 0.7016 (Y:0.01, G:0.69) | PEHE: 0.0038 (Best: 0.0038)\n","[Dragonnet] Ep 214 | Loss: 0.7014 (Y:0.01, G:0.69) | PEHE: 0.0060 (Best: 0.0038)\n","Early stopping at epoch 214\n","[SAVE] Run 0 complete. Test PEHE: 0.0059\n","\n","[TRAIN DRAGONNET] beta=2.0, l=1, run=0\n","[Dragonnet] Ep 50 | Loss: 11.5963 (Y:10.91, G:0.68) | PEHE: 4.4679 (Best: 3.7606)\n","[Dragonnet] Ep 52 | Loss: 11.5056 (Y:10.82, G:0.68) | PEHE: 4.8635 (Best: 3.7606)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 4.8636\n","\n","[TRAIN DRAGONNET] beta=2.0, l=2, run=0\n","[Dragonnet] Ep 50 | Loss: 18.9568 (Y:18.27, G:0.69) | PEHE: 7.5944 (Best: 6.5861)\n","[Dragonnet] Ep 52 | Loss: 18.9529 (Y:18.27, G:0.69) | PEHE: 7.5866 (Best: 6.5861)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 7.5836\n","\n","[TRAIN DRAGONNET] beta=2.0, l=4, run=0\n","[Dragonnet] Ep 50 | Loss: 31.7441 (Y:31.06, G:0.69) | PEHE: 10.9185 (Best: 10.0933)\n","[Dragonnet] Ep 60 | Loss: 31.6608 (Y:30.97, G:0.69) | PEHE: 11.9540 (Best: 10.0933)\n","Early stopping at epoch 60\n","[SAVE] Run 0 complete. Test PEHE: 11.9548\n","\n","[TRAIN DRAGONNET] beta=2.0, l=8, run=0\n","[Dragonnet] Ep 50 | Loss: 59.6719 (Y:58.98, G:0.69) | PEHE: 15.5590 (Best: 15.0283)\n","[Dragonnet] Ep 85 | Loss: 59.1790 (Y:58.49, G:0.69) | PEHE: 16.9245 (Best: 15.0283)\n","Early stopping at epoch 85\n","[SAVE] Run 0 complete. Test PEHE: 16.9229\n","\n","[TRAIN DRAGONNET] beta=4.0, l=0, run=0\n","[Dragonnet] Ep 50 | Loss: 0.7066 (Y:0.02, G:0.69) | PEHE: 0.0115 (Best: 0.0038)\n","[Dragonnet] Ep 100 | Loss: 0.7041 (Y:0.01, G:0.69) | PEHE: 0.0148 (Best: 0.0038)\n","[Dragonnet] Ep 106 | Loss: 0.7037 (Y:0.01, G:0.69) | PEHE: 0.0232 (Best: 0.0038)\n","Early stopping at epoch 106\n","[SAVE] Run 0 complete. Test PEHE: 0.0231\n","\n","[TRAIN DRAGONNET] beta=4.0, l=1, run=0\n","[Dragonnet] Ep 50 | Loss: 32.3792 (Y:31.71, G:0.67) | PEHE: 11.7451 (Best: 10.6202)\n","[Dragonnet] Ep 100 | Loss: 32.3010 (Y:31.64, G:0.67) | PEHE: 11.4717 (Best: 10.3416)\n","[Dragonnet] Ep 139 | Loss: 32.2058 (Y:31.54, G:0.67) | PEHE: 11.3280 (Best: 10.3416)\n","Early stopping at epoch 139\n","[SAVE] Run 0 complete. Test PEHE: 11.3255\n","\n","[TRAIN DRAGONNET] beta=4.0, l=2, run=0\n","[Dragonnet] Ep 50 | Loss: 55.4977 (Y:54.82, G:0.68) | PEHE: 17.4278 (Best: 15.6211)\n","[Dragonnet] Ep 57 | Loss: 55.2638 (Y:54.59, G:0.68) | PEHE: 16.2506 (Best: 15.6211)\n","Early stopping at epoch 57\n","[SAVE] Run 0 complete. Test PEHE: 16.2490\n","\n","[TRAIN DRAGONNET] beta=4.0, l=4, run=0\n","[Dragonnet] Ep 50 | Loss: 106.5579 (Y:105.87, G:0.69) | PEHE: 25.4736 (Best: 22.8257)\n","[Dragonnet] Ep 100 | Loss: 105.1421 (Y:104.46, G:0.69) | PEHE: 24.1597 (Best: 21.4510)\n","[Dragonnet] Ep 144 | Loss: 104.3333 (Y:103.65, G:0.68) | PEHE: 22.7096 (Best: 21.4510)\n","Early stopping at epoch 144\n","[SAVE] Run 0 complete. Test PEHE: 22.8313\n","\n","[TRAIN DRAGONNET] beta=4.0, l=8, run=0\n","[Dragonnet] Ep 50 | Loss: 210.8592 (Y:210.17, G:0.69) | PEHE: 35.7804 (Best: 27.4356)\n","[Dragonnet] Ep 52 | Loss: 211.0044 (Y:210.31, G:0.69) | PEHE: 34.3338 (Best: 27.4356)\n","Early stopping at epoch 52\n","[SAVE] Run 0 complete. Test PEHE: 34.3314\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Qghm-emgkuxt"},"execution_count":null,"outputs":[]}]}